{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_2_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/5/2/5_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp_1wZ827E-W",
        "colab_type": "text"
      },
      "source": [
        "##批次归一化\n",
        "批次归一化是一种提高神经网络性能和稳定性的技术。想法是归一化层输入，使它们的均值为 0 和方差为 1，类似于我们归一化网络输入的方法。批归一化对于 DCGAN 正常运作很必要。\n",
        "\n",
        "我们准备了一些 notebook 供你参考学习批归一化和如何在 TensorFlow 中实现它。跟往常一样，你可以在我们的 [GitHub 代码库](https://github.com/udacity/cn-deep-learning) 中找到 notebook，它们位于 tutorials/batch-norm 文件夹下。如果你已克隆了代码库，可执行 git pull 获取新文件。如果没有，请先克隆代码库："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS__SeeI7LQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "git clone https://github.com/udacity/deep-learning.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te3NuHTI7N3_",
        "colab_type": "text"
      },
      "source": [
        "或者，你也可以[在此](https://github.com/udacity/cn-deep-learning/tree/master/tutorials/batch-norm) 获取 notebook。\n",
        "\n",
        "你会找到三个 notebook：\n",
        "\n",
        "*   Batch_Normalization_Lesson.ipynb - 此 notebook 将向你展示批次归一化的原理\n",
        "*   Batch_Normalization_Exercises.ipynb - 由你来实现批次归一化的练习\n",
        "*   Batch_Normalization_Solutions.ipynb - 这些练习的解决方案\n",
        "\n",
        "\n"
      ]
    }
  ]
}