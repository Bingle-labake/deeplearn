{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_7_0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/2/7/2_7_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnCOlgGSdvXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S0SXf0vd05Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb1upjfzd5Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/drive/colab/2_7_0/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xlyof7_ThZP",
        "colab_type": "text"
      },
      "source": [
        "##ä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ\n",
        "åœ¨æ­¤é¡¹ç›®ä¸­ï¼Œä½ å°†æ„å»ºä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå¹¶ç”¨è¯¥ç½‘ç»œé¢„æµ‹æ¯æ—¥è‡ªè¡Œè½¦ç§Ÿå®¢äººæ•°ã€‚æˆ‘ä»¬æä¾›äº†ä¸€äº›ä»£ç ï¼Œä½†æ˜¯éœ€è¦ä½ æ¥å®ç°ç¥ç»ç½‘ç»œï¼ˆå¤§éƒ¨åˆ†å†…å®¹ï¼‰ã€‚æäº¤æ­¤é¡¹ç›®åï¼Œæ¬¢è¿è¿›ä¸€æ­¥æ¢ç´¢è¯¥æ•°æ®å’Œæ¨¡å‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLtLJvJXTNEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo4tuRdyTrY7",
        "colab_type": "text"
      },
      "source": [
        "##åŠ è½½å’Œå‡†å¤‡æ•°æ®\n",
        "æ„å»ºç¥ç»ç½‘ç»œçš„å…³é”®ä¸€æ­¥æ˜¯æ­£ç¡®åœ°å‡†å¤‡æ•°æ®ã€‚ä¸åŒå°ºåº¦çº§åˆ«çš„å˜é‡ä½¿ç½‘ç»œéš¾ä»¥é«˜æ•ˆåœ°æŒæ¡æ­£ç¡®çš„æƒé‡ã€‚æˆ‘ä»¬åœ¨ä¸‹æ–¹å·²ç»æä¾›äº†åŠ è½½å’Œå‡†å¤‡æ•°æ®çš„ä»£ç ã€‚ä½ å¾ˆå¿«å°†è¿›ä¸€æ­¥å­¦ä¹ è¿™äº›ä»£ç ï¼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoQczTrYTwGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
        "\n",
        "rides = pd.read_csv(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eg3-ZhwTzE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rides.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkpjBt7RT2Vf",
        "colab_type": "text"
      },
      "source": [
        "##æ•°æ®ç®€ä»‹\n",
        "æ­¤æ•°æ®é›†åŒ…å«çš„æ˜¯ä» 2011 å¹´ 1 æœˆ 1 æ—¥åˆ° 2012 å¹´ 12 æœˆ 31 æ—¥æœŸé—´æ¯å¤©æ¯å°æ—¶çš„éª‘è½¦äººæ•°ã€‚éª‘è½¦ç”¨æˆ·åˆ†æˆä¸´æ—¶ç”¨æˆ·å’Œæ³¨å†Œç”¨æˆ·ï¼Œcnt åˆ—æ˜¯éª‘è½¦ç”¨æˆ·æ•°æ±‡æ€»åˆ—ã€‚ä½ å¯ä»¥åœ¨ä¸Šæ–¹çœ‹åˆ°å‰å‡ è¡Œæ•°æ®ã€‚\n",
        "\n",
        "ä¸‹å›¾å±•ç¤ºçš„æ˜¯æ•°æ®é›†ä¸­å‰ 10 å¤©å·¦å³çš„éª‘è½¦äººæ•°ï¼ˆæŸäº›å¤©ä¸ä¸€å®šæ˜¯ 24 ä¸ªæ¡ç›®ï¼Œæ‰€ä»¥ä¸æ˜¯ç²¾ç¡®çš„ 10 å¤©ï¼‰ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ¯å°æ—¶ç§Ÿé‡‘ã€‚è¿™äº›æ•°æ®å¾ˆå¤æ‚ï¼å‘¨æœ«çš„éª‘è¡Œäººæ•°å°‘äº›ï¼Œå·¥ä½œæ—¥ä¸Šä¸‹ç­æœŸé—´æ˜¯éª‘è¡Œé«˜å³°æœŸã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»ä¸Šæ–¹çš„æ•°æ®ä¸­çœ‹åˆ°æ¸©åº¦ã€æ¹¿åº¦å’Œé£é€Ÿä¿¡æ¯ï¼Œæ‰€æœ‰è¿™äº›ä¿¡æ¯éƒ½ä¼šå½±å“éª‘è¡Œäººæ•°ã€‚ä½ éœ€è¦ç”¨ä½ çš„æ¨¡å‹å±•ç¤ºæ‰€æœ‰è¿™äº›æ•°æ®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_HMj0Z5T303",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rides[:24*10].plot(x='dteday', y='cnt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_aLWBCIT_H9",
        "colab_type": "text"
      },
      "source": [
        "##è™šæ‹Ÿå˜é‡ï¼ˆå“‘å˜é‡ï¼‰\n",
        "ä¸‹é¢æ˜¯ä¸€äº›åˆ†ç±»å˜é‡ï¼Œä¾‹å¦‚å­£èŠ‚ã€å¤©æ°”ã€æœˆä»½ã€‚è¦åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­åŒ…å«è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºäºŒè¿›åˆ¶è™šæ‹Ÿå˜é‡ã€‚ç”¨ Pandas åº“ä¸­çš„ get_dummies() å°±å¯ä»¥è½»æ¾å®ç°ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEBHD8LiT-Yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
        "for each in dummy_fields:\n",
        "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
        "    rides = pd.concat([rides, dummies], axis=1)\n",
        "\n",
        "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
        "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
        "data = rides.drop(fields_to_drop, axis=1)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdX5-a4UUGFb",
        "colab_type": "text"
      },
      "source": [
        "##è°ƒæ•´ç›®æ ‡å˜é‡\n",
        "ä¸ºäº†æ›´è½»æ¾åœ°è®­ç»ƒç½‘ç»œï¼Œæˆ‘ä»¬å°†å¯¹æ¯ä¸ªè¿ç»­å˜é‡æ ‡å‡†åŒ–ï¼Œå³è½¬æ¢å’Œè°ƒæ•´å˜é‡ï¼Œä½¿å®ƒä»¬çš„å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚\n",
        "\n",
        "\n",
        "\n",
        "æˆ‘ä»¬ä¼šä¿å­˜æ¢ç®—å› å­ï¼Œä»¥ä¾¿å½“æˆ‘ä»¬ä½¿ç”¨ç½‘ç»œè¿›è¡Œé¢„æµ‹æ—¶å¯ä»¥è¿˜åŸæ•°æ®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2qYXvakUKfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
        "# Store scalings in a dictionary so we can convert back later\n",
        "scaled_features = {}\n",
        "for each in quant_features:\n",
        "    mean, std = data[each].mean(), data[each].std()\n",
        "    scaled_features[each] = [mean, std]\n",
        "    data.loc[:, each] = (data[each] - mean)/std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOZWa71hUN4Q",
        "colab_type": "text"
      },
      "source": [
        "##å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒã€æµ‹è¯•å’ŒéªŒè¯æ•°æ®é›†\n",
        "\n",
        "æˆ‘ä»¬å°†å¤§çº¦æœ€å 21 å¤©çš„æ•°æ®ä¿å­˜ä¸ºæµ‹è¯•æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ä¼šåœ¨è®­ç»ƒå®Œç½‘ç»œåä½¿ç”¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä¸å®é™…çš„éª‘è¡Œäººæ•°è¿›è¡Œå¯¹æ¯”ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDp4fuIKUWcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now remove the test data from the data set \n",
        "data = data[:-21*24]\n",
        "\n",
        "# Separate the data into features and targets\n",
        "target_fields = ['cnt', 'casual', 'registered']\n",
        "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
        "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYY9D4Z7UZJ9",
        "colab_type": "text"
      },
      "source": [
        "æˆ‘ä»¬å°†æ•°æ®æ‹†åˆ†ä¸ºä¸¤ä¸ªæ•°æ®é›†ï¼Œä¸€ä¸ªç”¨ä½œè®­ç»ƒï¼Œä¸€ä¸ªåœ¨ç½‘ç»œè®­ç»ƒå®Œåç”¨æ¥éªŒè¯ç½‘ç»œã€‚å› ä¸ºæ•°æ®æ˜¯æœ‰æ—¶é—´åºåˆ—ç‰¹æ€§çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨å†å²æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç„¶åå°è¯•é¢„æµ‹æœªæ¥æ•°æ®ï¼ˆéªŒè¯æ•°æ®é›†ï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCixFk3aUcB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hold out the last 60 days or so of the remaining data as a validation set\n",
        "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
        "val_features, val_targets = features[-60*24:], targets[-60*24:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SreRg4vjUfBU",
        "colab_type": "text"
      },
      "source": [
        "##å¼€å§‹æ„å»ºç½‘ç»œ\n",
        "ä¸‹é¢ä½ å°†æ„å»ºè‡ªå·±çš„ç½‘ç»œã€‚æˆ‘ä»¬å·²ç»æ„å»ºå¥½ç»“æ„å’Œåå‘ä¼ é€’éƒ¨åˆ†ã€‚ä½ å°†å®ç°ç½‘ç»œçš„å‰å‘ä¼ é€’éƒ¨åˆ†ã€‚è¿˜éœ€è¦è®¾ç½®è¶…å‚æ•°ï¼šå­¦ä¹ é€Ÿç‡ã€éšè—å•å…ƒçš„æ•°é‡ï¼Œä»¥åŠè®­ç»ƒä¼ é€’æ•°é‡ã€‚\n",
        "\n",
        "![æ›¿ä»£æ–‡å­—](http://p6.qhimg.com/t013b6b19e2bbbe62bc.png)\n",
        "\n",
        "è¯¥ç½‘ç»œæœ‰ä¸¤ä¸ªå±‚çº§ï¼Œä¸€ä¸ªéšè—å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ã€‚éšè—å±‚çº§å°†ä½¿ç”¨ S å‹å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚è¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œç”¨äºé€’å½’ï¼ŒèŠ‚ç‚¹çš„è¾“å‡ºå’ŒèŠ‚ç‚¹çš„è¾“å…¥ç›¸åŒã€‚å³æ¿€æ´»å‡½æ•°æ˜¯  ğ‘“(ğ‘¥)=ğ‘¥f(x)=x ã€‚è¿™ç§å‡½æ•°è·å¾—è¾“å…¥ä¿¡å·ï¼Œå¹¶ç”Ÿæˆè¾“å‡ºä¿¡å·ï¼Œä½†æ˜¯ä¼šè€ƒè™‘é˜ˆå€¼ï¼Œç§°ä¸ºæ¿€æ´»å‡½æ•°ã€‚æˆ‘ä»¬å®Œæˆç½‘ç»œçš„æ¯ä¸ªå±‚çº§ï¼Œå¹¶è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºã€‚ä¸€ä¸ªå±‚çº§çš„æ‰€æœ‰è¾“å‡ºå˜æˆä¸‹ä¸€å±‚çº§ç¥ç»å…ƒçš„è¾“å…¥ã€‚è¿™ä¸€æµç¨‹å«åšå‰å‘ä¼ æ’­ï¼ˆforward propagationï¼‰ã€‚\n",
        "\n",
        "\n",
        "æˆ‘ä»¬åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨æƒé‡å°†ä¿¡å·ä»è¾“å…¥å±‚ä¼ æ’­åˆ°è¾“å‡ºå±‚ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æƒé‡å°†é”™è¯¯ä»è¾“å‡ºå±‚ä¼ æ’­å›ç½‘ç»œï¼Œä»¥ä¾¿æ›´æ–°æƒé‡ã€‚è¿™å«åšåå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰ã€‚\n",
        "\n",
        "\n",
        "> **æç¤º**ï¼šä½ éœ€è¦ä¸ºåå‘ä¼ æ’­å®ç°è®¡ç®—è¾“å‡ºæ¿€æ´»å‡½æ•° ( ğ‘“(ğ‘¥)=ğ‘¥f(x)=x ) çš„å¯¼æ•°ã€‚å¦‚æœä½ ä¸ç†Ÿæ‚‰å¾®ç§¯åˆ†ï¼Œå…¶å®è¯¥å‡½æ•°å°±ç­‰åŒäºç­‰å¼  ğ‘¦=ğ‘¥y=x ã€‚è¯¥ç­‰å¼çš„æ–œç‡æ˜¯å¤šå°‘ï¼Ÿä¹Ÿå°±æ˜¯å¯¼æ•°  ğ‘“(ğ‘¥)f(x) ã€‚\n",
        "\n",
        "\n",
        "ä½ éœ€è¦å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n",
        "\n",
        "\n",
        "1.   å®ç° S å‹æ¿€æ´»å‡½æ•°ã€‚å°† __init__ ä¸­çš„ self.activation_function è®¾ä¸ºä½ çš„ S å‹å‡½æ•°ã€‚\n",
        "2.   åœ¨ train æ–¹æ³•ä¸­å®ç°å‰å‘ä¼ é€’ã€‚\n",
        "3.   åœ¨ train æ–¹æ³•ä¸­å®ç°åå‘ä¼ æ’­ç®—æ³•ï¼ŒåŒ…æ‹¬è®¡ç®—è¾“å‡ºé”™è¯¯ã€‚\n",
        "4.   åœ¨ run æ–¹æ³•ä¸­å®ç°å‰å‘ä¼ é€’ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5KzRYsgVgHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "        # Set number of nodes in input, hidden and output layers.\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
        "                                       (self.input_nodes, self.hidden_nodes))\n",
        "\n",
        "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
        "                                       (self.hidden_nodes, self.output_nodes))\n",
        "        self.lr = learning_rate\n",
        "        \n",
        "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\n",
        "        #\n",
        "        # Note: in Python, you can define a function with a lambda expression,\n",
        "        # as shown below.\n",
        "        #self.activation_function = lambda x : None  # Replace 0 with your sigmoid calculation.\n",
        "        self.activation_function = lambda x:1/(1 + np.exp(-x))\n",
        "        \n",
        "        ### If the lambda code above is not something you're familiar with,\n",
        "        # You can uncomment out the following three lines and put your \n",
        "        # implementation there instead.\n",
        "        #\n",
        "        #def sigmoid(x):\n",
        "        #    return 0  # Replace 0 with your sigmoid calculation here\n",
        "        #self.activation_function = sigmoid        \n",
        "        #self.sigmoid_derivate = lambda x:x*(1 - x)\n",
        "                    \n",
        "    \n",
        "    def train(self, features, targets):\n",
        "        ''' Train the network on batch of features and targets. \n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            \n",
        "            features: 2D array, each row is one data record, each column is a feature\n",
        "            targets: 1D array of target values\n",
        "        \n",
        "        '''\n",
        "        n_records = features.shape[0]\n",
        "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
        "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
        "        for X, y in zip(features, targets):\n",
        "            #### Implement the forward pass here ####\n",
        "            ### Forward pass ###\n",
        "            # TODO: Hidden layer - Replace these values with your calculations.\n",
        "            hidden_inputs = np.dot(X,self.weights_input_to_hidden) # signals into hidden layer\n",
        "            hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
        "\n",
        "            # TODO: Output layer - Replace these values with your calculations.\n",
        "            final_inputs = np.dot(hidden_outputs,self.weights_hidden_to_output) # signals into final output layer\n",
        "            final_outputs = final_inputs # signals from final output layer\n",
        "            \n",
        "            #### Implement the backward pass here ####\n",
        "            ### Backward pass ###\n",
        "\n",
        "            # TODO: Output error - Replace this value with your calculations.\n",
        "            error = y-final_outputs # Output layer error is the difference between desired target and actual output.\n",
        "            \n",
        "            # TODO: Calculate the hidden layer's contribution to the error\n",
        "            hidden_error = error*self.weights_hidden_to_output\n",
        "            \n",
        "            # TODO: Backpropagated error terms - Replace these values with your calculations.\n",
        "            output_error_term = error\n",
        "            hidden_error_term = hidden_error.T * hidden_outputs * (1 - hidden_outputs)\n",
        "\n",
        "            # Weight step (input to hidden)\n",
        "            delta_weights_i_h += hidden_error_term * X[:,None]\n",
        "            # Weight step (hidden to output)\n",
        "            delta_weights_h_o += output_error_term * hidden_outputs[:,None]\n",
        "\n",
        "        # TODO: Update the weights - Replace these values with your calculations.\n",
        "        self.weights_hidden_to_output += self.lr * delta_weights_h_o / n_records # update hidden-to-output weights with gradient descent step\n",
        "        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records # update input-to-hidden weights with gradient descent step\n",
        " \n",
        "    def run(self, features):\n",
        "        ''' Run a forward pass through the network with input features \n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            features: 1D array of feature values\n",
        "        '''\n",
        "        \n",
        "        #### Implement the forward pass here ####\n",
        "        # TODO: Hidden layer - replace these values with the appropriate calculations.\n",
        "        hidden_inputs = np.dot(features, self.weights_input_to_hidden) # signals into hidden layer\n",
        "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
        "        \n",
        "        # TODO: Output layer - Replace these values with the appropriate calculations.\n",
        "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
        "        final_outputs = final_inputs # signals from final output layer \n",
        "        \n",
        "        return final_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTDmOJ40VhEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MSE(y, Y):\n",
        "    return np.mean((y-Y)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcqUND4sVkrI",
        "colab_type": "text"
      },
      "source": [
        "##å•å…ƒæµ‹è¯•\n",
        "è¿è¡Œè¿™äº›å•å…ƒæµ‹è¯•ï¼Œæ£€æŸ¥ä½ çš„ç½‘ç»œå®ç°æ˜¯å¦æ­£ç¡®ã€‚è¿™æ ·å¯ä»¥å¸®åŠ©ä½ ç¡®ä¿ç½‘ç»œå·²æ­£ç¡®å®ç°ï¼Œç„¶åå†å¼€å§‹è®­ç»ƒç½‘ç»œã€‚è¿™äº›æµ‹è¯•å¿…é¡»æˆåŠŸæ‰èƒ½é€šè¿‡æ­¤é¡¹ç›®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpPIpGiuVoZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unittest\n",
        "\n",
        "inputs = np.array([[0.5, -0.2, 0.1]])\n",
        "targets = np.array([[0.4]])\n",
        "test_w_i_h = np.array([[0.1, -0.2],\n",
        "                       [0.4, 0.5],\n",
        "                       [-0.3, 0.2]])\n",
        "test_w_h_o = np.array([[0.3],\n",
        "                       [-0.1]])\n",
        "\n",
        "class TestMethods(unittest.TestCase):\n",
        "    \n",
        "    ##########\n",
        "    # Unit tests for data loading\n",
        "    ##########\n",
        "    \n",
        "    def test_data_path(self):\n",
        "        # Test that file path to dataset has been unaltered\n",
        "        self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv')\n",
        "        \n",
        "    def test_data_loaded(self):\n",
        "        # Test that data frame loaded\n",
        "        self.assertTrue(isinstance(rides, pd.DataFrame))\n",
        "    \n",
        "    ##########\n",
        "    # Unit tests for network functionality\n",
        "    ##########\n",
        "\n",
        "    def test_activation(self):\n",
        "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
        "        # Test that the activation function is a sigmoid\n",
        "        self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))\n",
        "\n",
        "    def test_train(self):\n",
        "        # Test that weights are updated correctly on training\n",
        "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
        "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
        "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
        "        \n",
        "        network.train(inputs, targets)\n",
        "        self.assertTrue(np.allclose(network.weights_hidden_to_output, \n",
        "                                    np.array([[ 0.37275328], \n",
        "                                              [-0.03172939]])))\n",
        "        self.assertTrue(np.allclose(network.weights_input_to_hidden,\n",
        "                                    np.array([[ 0.10562014, -0.20185996], \n",
        "                                              [0.39775194, 0.50074398], \n",
        "                                              [-0.29887597, 0.19962801]])))\n",
        "\n",
        "    def test_run(self):\n",
        "        # Test correctness of run method\n",
        "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
        "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
        "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
        "\n",
        "        self.assertTrue(np.allclose(network.run(inputs), 0.09998924))\n",
        "\n",
        "suite = unittest.TestLoader().loadTestsFromModule(TestMethods())\n",
        "unittest.TextTestRunner().run(suite)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpLLU7CAdMUi",
        "colab_type": "text"
      },
      "source": [
        "##è®­ç»ƒç½‘ç»œ\n",
        "ç°åœ¨ä½ å°†è®¾ç½®ç½‘ç»œçš„è¶…å‚æ•°ã€‚ç­–ç•¥æ˜¯è®¾ç½®çš„è¶…å‚æ•°ä½¿è®­ç»ƒé›†ä¸Šçš„é”™è¯¯å¾ˆå°ä½†æ˜¯æ•°æ®ä¸ä¼šè¿‡æ‹Ÿåˆã€‚å¦‚æœç½‘ç»œè®­ç»ƒæ—¶é—´å¤ªé•¿ï¼Œæˆ–è€…æœ‰å¤ªå¤šçš„éšè—èŠ‚ç‚¹ï¼Œå¯èƒ½å°±ä¼šè¿‡äºé’ˆå¯¹ç‰¹å®šè®­ç»ƒé›†ï¼Œæ— æ³•æ³›åŒ–åˆ°éªŒè¯æ•°æ®é›†ã€‚å³å½“è®­ç»ƒé›†çš„æŸå¤±é™ä½æ—¶ï¼ŒéªŒè¯é›†çš„æŸå¤±å°†å¼€å§‹å¢å¤§ã€‚\n",
        "\n",
        "ä½ è¿˜å°†é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ (SGD) æ–¹æ³•è®­ç»ƒç½‘ç»œã€‚å¯¹äºæ¯æ¬¡è®­ç»ƒï¼Œéƒ½è·å–éšæœºæ ·æœ¬æ•°æ®ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚ä¸æ™®é€šæ¢¯åº¦ä¸‹é™ç›¸æ¯”ï¼Œè®­ç»ƒæ¬¡æ•°è¦æ›´å¤šï¼Œä½†æ˜¯æ¯æ¬¡æ—¶é—´æ›´çŸ­ã€‚è¿™æ ·çš„è¯ï¼Œç½‘ç»œè®­ç»ƒæ•ˆç‡æ›´é«˜ã€‚ç¨åä½ å°†è¯¦ç»†äº†è§£ SGDã€‚\n",
        "\n",
        "##é€‰æ‹©è¿­ä»£æ¬¡æ•°\n",
        "ä¹Ÿå°±æ˜¯è®­ç»ƒç½‘ç»œæ—¶ä»è®­ç»ƒæ•°æ®ä¸­æŠ½æ ·çš„æ‰¹æ¬¡æ•°é‡ã€‚è¿­ä»£æ¬¡æ•°è¶Šå¤šï¼Œæ¨¡å‹å°±ä¸æ•°æ®è¶Šæ‹Ÿåˆã€‚ä½†æ˜¯ï¼Œå¦‚æœè¿­ä»£æ¬¡æ•°å¤ªå¤šï¼Œæ¨¡å‹å°±æ— æ³•å¾ˆå¥½åœ°æ³›åŒ–åˆ°å…¶ä»–æ•°æ®ï¼Œè¿™å«åšè¿‡æ‹Ÿåˆã€‚ä½ éœ€è¦é€‰æ‹©ä¸€ä¸ªä½¿è®­ç»ƒæŸå¤±å¾ˆä½å¹¶ä¸”éªŒè¯æŸå¤±ä¿æŒä¸­ç­‰æ°´å¹³çš„æ•°å­—ã€‚å½“ä½ å¼€å§‹è¿‡æ‹Ÿåˆæ—¶ï¼Œä½ ä¼šå‘ç°è®­ç»ƒæŸå¤±ç»§ç»­ä¸‹é™ï¼Œä½†æ˜¯éªŒè¯æŸå¤±å¼€å§‹ä¸Šå‡ã€‚\n",
        "\n",
        "##é€‰æ‹©å­¦ä¹ é€Ÿç‡\n",
        "é€Ÿç‡å¯ä»¥è°ƒæ•´æƒé‡æ›´æ–°å¹…åº¦ã€‚å¦‚æœé€Ÿç‡å¤ªå¤§ï¼Œæƒé‡å°±ä¼šå¤ªå¤§ï¼Œå¯¼è‡´ç½‘ç»œæ— æ³•ä¸æ•°æ®ç›¸æ‹Ÿåˆã€‚å»ºè®®ä» 0.1 å¼€å§‹ã€‚å¦‚æœç½‘ç»œåœ¨ä¸æ•°æ®æ‹Ÿåˆæ—¶é‡åˆ°é—®é¢˜ï¼Œå°è¯•é™ä½å­¦ä¹ é€Ÿç‡ã€‚æ³¨æ„ï¼Œå­¦ä¹ é€Ÿç‡è¶Šä½ï¼Œæƒé‡æ›´æ–°çš„æ­¥é•¿å°±è¶Šå°ï¼Œç¥ç»ç½‘ç»œæ”¶æ•›çš„æ—¶é—´å°±è¶Šé•¿ã€‚\n",
        "\n",
        "##é€‰æ‹©éšè—èŠ‚ç‚¹æ•°é‡\n",
        "éšè—èŠ‚ç‚¹è¶Šå¤šï¼Œæ¨¡å‹çš„é¢„æµ‹ç»“æœå°±è¶Šå‡†ç¡®ã€‚å°è¯•ä¸åŒçš„éšè—èŠ‚ç‚¹çš„æ•°é‡ï¼Œçœ‹çœ‹å¯¹æ€§èƒ½æœ‰ä½•å½±å“ã€‚ä½ å¯ä»¥æŸ¥çœ‹æŸå¤±å­—å…¸ï¼Œå¯»æ‰¾ç½‘ç»œæ€§èƒ½æŒ‡æ ‡ã€‚å¦‚æœéšè—å•å…ƒçš„æ•°é‡å¤ªå°‘ï¼Œé‚£ä¹ˆæ¨¡å‹å°±æ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´è¿›è¡Œå­¦ä¹ ï¼Œå¦‚æœå¤ªå¤šï¼Œåˆ™å­¦ä¹ æ–¹å‘å°±æœ‰å¤ªå¤šçš„é€‰æ‹©ã€‚é€‰æ‹©éšè—å•å…ƒæ•°é‡çš„æŠ€å·§åœ¨äºæ‰¾åˆ°åˆé€‚çš„å¹³è¡¡ç‚¹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXgI-nordVJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "### Set the hyperparameters here ###\n",
        "iterations = 3000\n",
        "learning_rate = 0.56\n",
        "hidden_nodes = 25\n",
        "output_nodes = 1\n",
        "\n",
        "N_i = train_features.shape[1]\n",
        "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
        "\n",
        "losses = {'train':[], 'validation':[]}\n",
        "for ii in range(iterations):\n",
        "    # Go through a random batch of 128 records from the training data set\n",
        "    batch = np.random.choice(train_features.index, size=128)\n",
        "    X, y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']\n",
        "                             \n",
        "    network.train(X, y)\n",
        "    \n",
        "    # Printing out the training progress\n",
        "    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n",
        "    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n",
        "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
        "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
        "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    losses['train'].append(train_loss)\n",
        "    losses['validation'].append(val_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbcLPXwWdZPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(losses['train'], label='Training loss')\n",
        "plt.plot(losses['validation'], label='Validation loss')\n",
        "plt.legend()\n",
        "_ = plt.ylim()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8weGHgUdcpW",
        "colab_type": "text"
      },
      "source": [
        "##æ£€æŸ¥é¢„æµ‹ç»“æœ\n",
        "ä½¿ç”¨æµ‹è¯•æ•°æ®çœ‹çœ‹ç½‘ç»œå¯¹æ•°æ®å»ºæ¨¡çš„æ•ˆæœå¦‚ä½•ã€‚å¦‚æœå®Œå…¨é”™äº†ï¼Œè¯·ç¡®ä¿ç½‘ç»œä¸­çš„æ¯æ­¥éƒ½æ­£ç¡®å®ç°ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69yCra-BdgRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,4))\n",
        "\n",
        "mean, std = scaled_features['cnt']\n",
        "predictions = network.run(test_features).T*std + mean\n",
        "ax.plot(predictions[0], label='Prediction')\n",
        "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
        "ax.set_xlim(right=len(predictions))\n",
        "ax.legend()\n",
        "\n",
        "dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])\n",
        "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
        "ax.set_xticks(np.arange(len(dates))[12::24])\n",
        "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKXBEnJ5dknz",
        "colab_type": "text"
      },
      "source": [
        "##å¯é€‰ï¼šæ€è€ƒä¸‹ä½ çš„ç»“æœï¼ˆæˆ‘ä»¬ä¸ä¼šè¯„ä¼°è¿™é“é¢˜çš„ç­”æ¡ˆï¼‰\n",
        "è¯·é’ˆå¯¹ä½ çš„ç»“æœå›ç­”ä»¥ä¸‹é—®é¢˜ã€‚æ¨¡å‹å¯¹æ•°æ®çš„é¢„æµ‹æ•ˆæœå¦‚ä½•ï¼Ÿå“ªé‡Œå‡ºç°é—®é¢˜äº†ï¼Ÿä¸ºä½•å‡ºç°é—®é¢˜å‘¢ï¼Ÿ\n",
        "\n",
        "\n",
        "> **æ³¨æ„ï¼š**ä½ å¯ä»¥é€šè¿‡åŒå‡»è¯¥å•å…ƒç¼–è¾‘æ–‡æœ¬ã€‚å¦‚æœæƒ³è¦é¢„è§ˆæ–‡æœ¬ï¼Œè¯·æŒ‰ Control + Enter\n",
        "\n",
        "\n",
        "è¯·å°†ä½ çš„ç­”æ¡ˆå¡«å†™åœ¨ä¸‹æ–¹\n",
        "é¢„æµ‹çš„æ•°å€¼ç¨å¾®åé«˜ï¼Œç‰¹åˆ«æ˜¯é¢„æµ‹å³°å€¼çš„æ—¶å€™ï¼›å¯èƒ½æ˜¯å­¦ä¹ å› å­åå¤§å¯¼è‡´ã€‚"
      ]
    }
  ]
}