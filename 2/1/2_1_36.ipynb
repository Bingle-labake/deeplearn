{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_1_36.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/2/1/2_1_36.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-UU32C44fz",
        "colab_type": "text"
      },
      "source": [
        "##利用神经网络来预测学生录取情况\n",
        "在该 notebook 中，我们基于以下三条数据预测了加州大学洛杉矶分校 (UCLA) 的研究生录取情况：\n",
        "\n",
        "\n",
        "*   GRE 分数（测试）即 GRE Scores (Test)\n",
        "*   GPA 分数（成绩）即 GPA Scores (Grades)\n",
        "*   评级（1-4）即 Class rank (1-4)\n",
        "\n",
        "数据集来源： http://www.ats.ucla.edu/\n",
        "\n",
        "###加载数据\n",
        "为了加载数据并很好地进行格式化，我们将使用两个非常有用的包，即 Pandas 和 Numpy。 你可以在这里阅读文档：\n",
        "\n",
        "*   https://pandas.pydata.org/pandas-docs/stable/\n",
        "*   https://docs.scipy.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fERkE6dA40z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing pandas and numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reading the csv file into a pandas DataFrame\n",
        "data = pd.read_csv('student_data.csv')\n",
        "\n",
        "# Printing out the first 10 rows of our data\n",
        "data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za3ixO8M5PZK",
        "colab_type": "text"
      },
      "source": [
        "###绘制数据\n",
        "首先让我们对数据进行绘图，看看它是什么样的。为了绘制二维图，让我们先忽略评级 (rank)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3TWpS6Y5TEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Function to help us plot\n",
        "def plot_points(data):\n",
        "    X = np.array(data[[\"gre\",\"gpa\"]])\n",
        "    y = np.array(data[\"admit\"])\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n",
        "    plt.xlabel('Test (GRE)')\n",
        "    plt.ylabel('Grades (GPA)')\n",
        "    \n",
        "# Plotting the points\n",
        "plot_points(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5VjhVBy5VtC",
        "colab_type": "text"
      },
      "source": [
        "粗略来说，它看起来像是，成绩 （grades) 和测试 (test) 分数高的学生通过了，而得分低的学生却没有，但数据并没有如我们所希望的那样，很好地分离。 也许将评级 (rank) 考虑进来会有帮助？ 接下来我们将绘制 4 个图，每个图代表一个级别。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbIOQ8l-5W7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separating the ranks\n",
        "data_rank1 = data[data[\"rank\"]==1]\n",
        "data_rank2 = data[data[\"rank\"]==2]\n",
        "data_rank3 = data[data[\"rank\"]==3]\n",
        "data_rank4 = data[data[\"rank\"]==4]\n",
        "\n",
        "# Plotting the graphs\n",
        "plot_points(data_rank1)\n",
        "plt.title(\"Rank 1\")\n",
        "plt.show()\n",
        "plot_points(data_rank2)\n",
        "plt.title(\"Rank 2\")\n",
        "plt.show()\n",
        "plot_points(data_rank3)\n",
        "plt.title(\"Rank 3\")\n",
        "plt.show()\n",
        "plot_points(data_rank4)\n",
        "plt.title(\"Rank 4\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTPrf0XG5br7",
        "colab_type": "text"
      },
      "source": [
        "现在看起来更棒啦，看上去评级越低，录取率越高。 让我们使用评级 (rank) 作为我们的输入之一。 为了做到这一点，我们应该对它进行一次one-hot 编码。\n",
        "\n",
        "###将评级进行 One-hot 编码\n",
        "我们将在 Pandas 中使用 get_dummies 函数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US8v6Rio5e6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:  Make dummy variables for rank\n",
        "one_hot_data = None\n",
        "\n",
        "# TODO: Drop the previous rank column\n",
        "one_hot_data = None\n",
        "\n",
        "# Print the first 10 rows of our data\n",
        "one_hot_data[:10]\n",
        "\n",
        "########################################\n",
        "\n",
        "\n",
        "# Make dummy variables for rank\n",
        "one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)\n",
        "\n",
        "# Drop the previous rank column\n",
        "one_hot_data = one_hot_data.drop('rank', axis=1)\n",
        "\n",
        "# Print the first 10 rows of our data\n",
        "one_hot_data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEtAsSnI5f4b",
        "colab_type": "text"
      },
      "source": [
        "###缩放数据\n",
        "下一步是缩放数据。 我们注意到成绩 (grades) 的范围是 1.0-4.0，而测试分数 （test scores) 的范围大概是 200-800，这个范围要大得多。 这意味着我们的数据存在偏差，使得神经网络很难处理。 让我们将两个特征放在 0-1 的范围内，将分数除以 4.0，将测试分数除以 800。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INhQw9Rc5kqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making a copy of our data\n",
        "processed_data = one_hot_data[:]\n",
        "\n",
        "# TODO: Scale the columns\n",
        "\n",
        "# Printing the first 10 rows of our procesed data\n",
        "processed_data[:10]\n",
        "\n",
        "##########################################\n",
        "\n",
        "# Copying our data\n",
        "processed_data = one_hot_data[:]\n",
        "\n",
        "# Scaling the columns\n",
        "processed_data['gre'] = processed_data['gre']/800\n",
        "processed_data['gpa'] = processed_data['gpa']/4.0\n",
        "processed_data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL9m9JcU5nYD",
        "colab_type": "text"
      },
      "source": [
        "###将数据分成训练集和测试集\n",
        "为了测试我们的算法，我们将数据分为训练集和测试集。 测试集的大小将占总数据的 10％。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTNpMDYo5qWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)\n",
        "train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample)\n",
        "\n",
        "print(\"Number of training samples is\", len(train_data))\n",
        "print(\"Number of testing samples is\", len(test_data))\n",
        "print(train_data[:10])\n",
        "print(test_data[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxhZVo1E5szE",
        "colab_type": "text"
      },
      "source": [
        "###将数据分成特征和目标（标签）\n",
        "现在，在培训前的最后一步，我们将把数据分为特征 (features)（X）和目标 (targets)（y）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2zI8g4Y5xDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = train_data.drop('admit', axis=1)\n",
        "targets = train_data['admit']\n",
        "features_test = test_data.drop('admit', axis=1)\n",
        "targets_test = test_data['admit']\n",
        "\n",
        "print(features[:10])\n",
        "print(targets[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xspxXFVq50A8",
        "colab_type": "text"
      },
      "source": [
        "###训练二层神经网络\n",
        "下列函数会训练二层神经网络。 首先，我们将写一些 helper 函数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbRZLG6951qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1-sigmoid(x))\n",
        "def error_formula(y, output):\n",
        "    return - y*np.log(output) - (1 - y) * np.log(1-output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYNxZtxw58Rt",
        "colab_type": "text"
      },
      "source": [
        "###误差反向传播\n",
        "现在轮到你来练习，编写误差项。 记住这是由方程\n",
        "(𝑦−𝑦̂ )\n",
        " \n",
        "给出的。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2DCsV36Axd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write the error term formula\n",
        "def error_term_formula(y, output):\n",
        "    pass\n",
        "  \n",
        "######################################\n",
        "  \n",
        "def error_term_formula(y, output):\n",
        "    return (y-output) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdYnX_H6Bq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network hyperparameters\n",
        "epochs = 1000\n",
        "learnrate = 0.5\n",
        "\n",
        "# Training function\n",
        "def train_nn(features, targets, epochs, learnrate):\n",
        "    \n",
        "    # Use to same seed to make debugging easier\n",
        "    np.random.seed(42)\n",
        "\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "\n",
        "    # Initialize weights\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features.values, targets):\n",
        "            # Loop through all records, x is the input, y is the target\n",
        "\n",
        "            # Activation of the output unit\n",
        "            #   Notice we multiply the inputs and the weights here \n",
        "            #   rather than storing h as a separate variable \n",
        "            output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "            # The error, the target minus the network output\n",
        "            error = error_formula(y, output)\n",
        "\n",
        "            # The error term\n",
        "            #   Notice we calulate f'(h) here instead of defining a separate\n",
        "            #   sigmoid_prime function. This just makes it faster because we\n",
        "            #   can re-use the result of the sigmoid function stored in\n",
        "            #   the output variable\n",
        "            error_term = error_term_formula(y, output)\n",
        "\n",
        "            # The gradient descent step, the error times the gradient times the inputs\n",
        "            del_w += error_term * x\n",
        "\n",
        "        # Update the weights here. The learning rate times the \n",
        "        # change in weights, divided by the number of records to average\n",
        "        weights += learnrate * del_w / n_records\n",
        "\n",
        "        # Printing out the error on the training set\n",
        "        if e % (epochs / 10) == 0:\n",
        "            out = sigmoid(np.dot(features, weights))\n",
        "            loss = np.mean((out - targets) ** 2)\n",
        "            print(\"Epoch:\", e)\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            print(\"=========\")\n",
        "    print(\"Finished training!\")\n",
        "    return weights\n",
        "    \n",
        "weights = train_nn(features, targets, epochs, learnrate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm4TiYY76GOV",
        "colab_type": "text"
      },
      "source": [
        "###计算测试 (Test) 数据的精确度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNec01ug6H4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate accuracy on test data\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}