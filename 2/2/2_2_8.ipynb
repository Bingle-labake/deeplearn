{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_2_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/2/2/2_2_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoIe79Jqs4JB",
        "colab_type": "text"
      },
      "source": [
        "##实现反向传播\n",
        "现在我们知道输出层的误差是\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/Kr90Hg.png)\n",
        "\n",
        "隐藏层误差是\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/Krp0YR.gif)\n",
        "\n",
        "\n",
        "现在我们只考虑一个简单神经网络，它只有一个隐藏层和一个输出节点。这是通过反向传播更新权重的算法概述：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KrpT6f.png)\n",
        "\n",
        "###反向传播练习\n",
        "现在你来实现一个通过反向传播训练的神经网络，数据集就是之前的研究生院录取数据。通过前面所学你现在有能力完成这个练习：\n",
        "\n",
        "你的目标是：\n",
        "\n",
        "*   实现一个正向传播\n",
        "*   实现反向传播算法\n",
        "*   更新权重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0JEdE3cuSt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#backprop.py#\n",
        "\n",
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "np.random.seed(21)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "n_hidden = 2  # number of hidden units\n",
        "epochs = 900\n",
        "learnrate = 0.005\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "# Initialize weights\n",
        "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                        size=(n_features, n_hidden))\n",
        "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                         size=n_hidden)\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
        "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        print(x)\n",
        "        ## Forward pass ##\n",
        "        # TODO: Calculate the output\n",
        "        hidden_input = np.dot(x, weights_input_hidden)\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "        output = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
        "\n",
        "        ## Backward pass ##\n",
        "        # TODO: Calculate the network's prediction error\n",
        "        error = y-output\n",
        "\n",
        "        # TODO: Calculate error term for the output unit\n",
        "        output_error_term = error * output * (1-output)\n",
        "\n",
        "        ## propagate errors to hidden layer\n",
        "\n",
        "        # TODO: Calculate the hidden layer's contribution to the error\n",
        "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
        "        \n",
        "        # TODO: Calculate the error term for the hidden layer\n",
        "        hidden_error_term = hidden_error * hidden_output * (1-hidden_output)\n",
        "        \n",
        "        # TODO: Update the change in weights\n",
        "        del_w_hidden_output += output_error_term * hidden_output\n",
        "        del_w_input_hidden += hidden_error_term * x[:,None]\n",
        "\n",
        "    # TODO: Update weights  (don't forget to division by n_records or number of samples)\n",
        "    weights_input_hidden += learnrate * del_w_input_hidden/n_records\n",
        "    weights_hidden_output += learnrate * del_w_hidden_output/n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
        "        out = sigmoid(np.dot(hidden_output,\n",
        "                             weights_hidden_output))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
        "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
        "predictions = out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ncHk_YubYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_prep.py#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "admissions = pd.read_csv('binary.csv')\n",
        "\n",
        "# Make dummy variables for rank\n",
        "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
        "data = data.drop('rank', axis=1)\n",
        "\n",
        "# Standarize features\n",
        "for field in ['gre', 'gpa']:\n",
        "    mean, std = data[field].mean(), data[field].std()\n",
        "    data.loc[:,field] = (data[field]-mean)/std\n",
        "    \n",
        "# Split off random 10% of the data for testing\n",
        "np.random.seed(21)\n",
        "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
        "data, test_data = data.ix[sample], data.drop(sample)\n",
        "\n",
        "# Split into features and targets\n",
        "features, targets = data.drop('admit', axis=1), data['admit']\n",
        "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg87SVBBuj_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution.py#\n",
        "\n",
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "np.random.seed(21)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "n_hidden = 2  # number of hidden units\n",
        "epochs = 900\n",
        "learnrate = 0.005\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "# Initialize weights\n",
        "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                        size=(n_features, n_hidden))\n",
        "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                         size=n_hidden)\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
        "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        ## Forward pass ##\n",
        "        # TODO: Calculate the output\n",
        "        hidden_input = np.dot(x, weights_input_hidden)\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "        output = sigmoid(np.dot(hidden_output,\n",
        "                                weights_hidden_output))\n",
        "\n",
        "        ## Backward pass ##\n",
        "        # TODO: Calculate the network's prediction error\n",
        "        error = y - output\n",
        "\n",
        "        # TODO: Calculate error term for the output unit\n",
        "        output_error_term = error * output * (1 - output)\n",
        "\n",
        "        ## propagate errors to hidden layer\n",
        "\n",
        "        # TODO: Calculate the hidden layer's contribution to the error\n",
        "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
        "\n",
        "        # TODO: Calculate the error term for the hidden layer\n",
        "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
        "\n",
        "        # TODO: Update the change in weights\n",
        "        del_w_hidden_output += output_error_term * hidden_output\n",
        "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
        "\n",
        "    # TODO: Update weights\n",
        "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
        "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
        "        out = sigmoid(np.dot(hidden_output,\n",
        "                             weights_hidden_output))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
        "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
        "predictions = out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmQb4NEqurtu",
        "colab_type": "text"
      },
      "source": [
        "binary.csv\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "admit,gre,gpa,rank\n",
        "0,380,3.61,3\n",
        "1,660,3.67,3\n",
        "1,800,4,1\n",
        "1,640,3.19,4\n",
        "0,520,2.93,4\n",
        "1,760,3,2\n",
        "1,560,2.98,1\n",
        "0,400,3.08,2\n",
        "1,540,3.39,3\n",
        "0,700,3.92,2\n",
        "0,800,4,4\n",
        "0,440,3.22,1\n",
        "1,760,4,1\n",
        "0,700,3.08,2\n",
        "1,700,4,1\n",
        "0,480,3.44,3\n",
        "0,780,3.87,4\n",
        "0,360,2.56,3\n",
        "0,800,3.75,2\n",
        "1,540,3.81,1\n",
        "0,500,3.17,3\n",
        "1,660,3.63,2\n",
        "0,600,2.82,4\n",
        "0,680,3.19,4\n",
        "1,760,3.35,2\n",
        "1,800,3.66,1\n",
        "1,620,3.61,1\n",
        "1,520,3.74,4\n",
        "1,780,3.22,2\n",
        "0,520,3.29,1\n",
        "0,540,3.78,4\n",
        "0,760,3.35,3\n",
        "0,600,3.4,3\n",
        "1,800,4,3\n",
        "0,360,3.14,1\n",
        "0,400,3.05,2\n",
        "0,580,3.25,1\n",
        "0,520,2.9,3\n",
        "1,500,3.13,2\n",
        "1,520,2.68,3\n",
        "0,560,2.42,2\n",
        "1,580,3.32,2\n",
        "1,600,3.15,2\n",
        "0,500,3.31,3\n",
        "0,700,2.94,2\n",
        "1,460,3.45,3\n",
        "1,580,3.46,2\n",
        "0,500,2.97,4\n",
        "0,440,2.48,4\n",
        "0,400,3.35,3\n",
        "0,640,3.86,3\n",
        "0,440,3.13,4\n",
        "0,740,3.37,4\n",
        "1,680,3.27,2\n",
        "0,660,3.34,3\n",
        "1,740,4,3\n",
        "0,560,3.19,3\n",
        "0,380,2.94,3\n",
        "0,400,3.65,2\n",
        "0,600,2.82,4\n",
        "1,620,3.18,2\n",
        "0,560,3.32,4\n",
        "0,640,3.67,3\n",
        "1,680,3.85,3\n",
        "0,580,4,3\n",
        "0,600,3.59,2\n",
        "0,740,3.62,4\n",
        "0,620,3.3,1\n",
        "0,580,3.69,1\n",
        "0,800,3.73,1\n",
        "0,640,4,3\n",
        "0,300,2.92,4\n",
        "0,480,3.39,4\n",
        "0,580,4,2\n",
        "0,720,3.45,4\n",
        "0,720,4,3\n",
        "0,560,3.36,3\n",
        "1,800,4,3\n",
        "0,540,3.12,1\n",
        "1,620,4,1\n",
        "0,700,2.9,4\n",
        "0,620,3.07,2\n",
        "0,500,2.71,2\n",
        "0,380,2.91,4\n",
        "1,500,3.6,3\n",
        "0,520,2.98,2\n",
        "0,600,3.32,2\n",
        "0,600,3.48,2\n",
        "0,700,3.28,1\n",
        "1,660,4,2\n",
        "0,700,3.83,2\n",
        "1,720,3.64,1\n",
        "0,800,3.9,2\n",
        "0,580,2.93,2\n",
        "1,660,3.44,2\n",
        "0,660,3.33,2\n",
        "0,640,3.52,4\n",
        "0,480,3.57,2\n",
        "0,700,2.88,2\n",
        "0,400,3.31,3\n",
        "0,340,3.15,3\n",
        "0,580,3.57,3\n",
        "0,380,3.33,4\n",
        "0,540,3.94,3\n",
        "1,660,3.95,2\n",
        "1,740,2.97,2\n",
        "1,700,3.56,1\n",
        "0,480,3.13,2\n",
        "0,400,2.93,3\n",
        "0,480,3.45,2\n",
        "0,680,3.08,4\n",
        "0,420,3.41,4\n",
        "0,360,3,3\n",
        "0,600,3.22,1\n",
        "0,720,3.84,3\n",
        "0,620,3.99,3\n",
        "1,440,3.45,2\n",
        "0,700,3.72,2\n",
        "1,800,3.7,1\n",
        "0,340,2.92,3\n",
        "1,520,3.74,2\n",
        "1,480,2.67,2\n",
        "0,520,2.85,3\n",
        "0,500,2.98,3\n",
        "0,720,3.88,3\n",
        "0,540,3.38,4\n",
        "1,600,3.54,1\n",
        "0,740,3.74,4\n",
        "0,540,3.19,2\n",
        "0,460,3.15,4\n",
        "1,620,3.17,2\n",
        "0,640,2.79,2\n",
        "0,580,3.4,2\n",
        "0,500,3.08,3\n",
        "0,560,2.95,2\n",
        "0,500,3.57,3\n",
        "0,560,3.33,4\n",
        "0,700,4,3\n",
        "0,620,3.4,2\n",
        "1,600,3.58,1\n",
        "0,640,3.93,2\n",
        "1,700,3.52,4\n",
        "0,620,3.94,4\n",
        "0,580,3.4,3\n",
        "0,580,3.4,4\n",
        "0,380,3.43,3\n",
        "0,480,3.4,2\n",
        "0,560,2.71,3\n",
        "1,480,2.91,1\n",
        "0,740,3.31,1\n",
        "1,800,3.74,1\n",
        "0,400,3.38,2\n",
        "1,640,3.94,2\n",
        "0,580,3.46,3\n",
        "0,620,3.69,3\n",
        "1,580,2.86,4\n",
        "0,560,2.52,2\n",
        "1,480,3.58,1\n",
        "0,660,3.49,2\n",
        "0,700,3.82,3\n",
        "0,600,3.13,2\n",
        "0,640,3.5,2\n",
        "1,700,3.56,2\n",
        "0,520,2.73,2\n",
        "0,580,3.3,2\n",
        "0,700,4,1\n",
        "0,440,3.24,4\n",
        "0,720,3.77,3\n",
        "0,500,4,3\n",
        "0,600,3.62,3\n",
        "0,400,3.51,3\n",
        "0,540,2.81,3\n",
        "0,680,3.48,3\n",
        "1,800,3.43,2\n",
        "0,500,3.53,4\n",
        "1,620,3.37,2\n",
        "0,520,2.62,2\n",
        "1,620,3.23,3\n",
        "0,620,3.33,3\n",
        "0,300,3.01,3\n",
        "0,620,3.78,3\n",
        "0,500,3.88,4\n",
        "0,700,4,2\n",
        "1,540,3.84,2\n",
        "0,500,2.79,4\n",
        "0,800,3.6,2\n",
        "0,560,3.61,3\n",
        "0,580,2.88,2\n",
        "0,560,3.07,2\n",
        "0,500,3.35,2\n",
        "1,640,2.94,2\n",
        "0,800,3.54,3\n",
        "0,640,3.76,3\n",
        "0,380,3.59,4\n",
        "1,600,3.47,2\n",
        "0,560,3.59,2\n",
        "0,660,3.07,3\n",
        "1,400,3.23,4\n",
        "0,600,3.63,3\n",
        "0,580,3.77,4\n",
        "0,800,3.31,3\n",
        "1,580,3.2,2\n",
        "1,700,4,1\n",
        "0,420,3.92,4\n",
        "1,600,3.89,1\n",
        "1,780,3.8,3\n",
        "0,740,3.54,1\n",
        "1,640,3.63,1\n",
        "0,540,3.16,3\n",
        "0,580,3.5,2\n",
        "0,740,3.34,4\n",
        "0,580,3.02,2\n",
        "0,460,2.87,2\n",
        "0,640,3.38,3\n",
        "1,600,3.56,2\n",
        "1,660,2.91,3\n",
        "0,340,2.9,1\n",
        "1,460,3.64,1\n",
        "0,460,2.98,1\n",
        "1,560,3.59,2\n",
        "0,540,3.28,3\n",
        "0,680,3.99,3\n",
        "1,480,3.02,1\n",
        "0,800,3.47,3\n",
        "0,800,2.9,2\n",
        "1,720,3.5,3\n",
        "0,620,3.58,2\n",
        "0,540,3.02,4\n",
        "0,480,3.43,2\n",
        "1,720,3.42,2\n",
        "0,580,3.29,4\n",
        "0,600,3.28,3\n",
        "0,380,3.38,2\n",
        "0,420,2.67,3\n",
        "1,800,3.53,1\n",
        "0,620,3.05,2\n",
        "1,660,3.49,2\n",
        "0,480,4,2\n",
        "0,500,2.86,4\n",
        "0,700,3.45,3\n",
        "0,440,2.76,2\n",
        "1,520,3.81,1\n",
        "1,680,2.96,3\n",
        "0,620,3.22,2\n",
        "0,540,3.04,1\n",
        "0,800,3.91,3\n",
        "0,680,3.34,2\n",
        "0,440,3.17,2\n",
        "0,680,3.64,3\n",
        "0,640,3.73,3\n",
        "0,660,3.31,4\n",
        "0,620,3.21,4\n",
        "1,520,4,2\n",
        "1,540,3.55,4\n",
        "1,740,3.52,4\n",
        "0,640,3.35,3\n",
        "1,520,3.3,2\n",
        "1,620,3.95,3\n",
        "0,520,3.51,2\n",
        "0,640,3.81,2\n",
        "0,680,3.11,2\n",
        "0,440,3.15,2\n",
        "1,520,3.19,3\n",
        "1,620,3.95,3\n",
        "1,520,3.9,3\n",
        "0,380,3.34,3\n",
        "0,560,3.24,4\n",
        "1,600,3.64,3\n",
        "1,680,3.46,2\n",
        "0,500,2.81,3\n",
        "1,640,3.95,2\n",
        "0,540,3.33,3\n",
        "1,680,3.67,2\n",
        "0,660,3.32,1\n",
        "0,520,3.12,2\n",
        "1,600,2.98,2\n",
        "0,460,3.77,3\n",
        "1,580,3.58,1\n",
        "1,680,3,4\n",
        "1,660,3.14,2\n",
        "0,660,3.94,2\n",
        "0,360,3.27,3\n",
        "0,660,3.45,4\n",
        "0,520,3.1,4\n",
        "1,440,3.39,2\n",
        "0,600,3.31,4\n",
        "1,800,3.22,1\n",
        "1,660,3.7,4\n",
        "0,800,3.15,4\n",
        "0,420,2.26,4\n",
        "1,620,3.45,2\n",
        "0,800,2.78,2\n",
        "0,680,3.7,2\n",
        "0,800,3.97,1\n",
        "0,480,2.55,1\n",
        "0,520,3.25,3\n",
        "0,560,3.16,1\n",
        "0,460,3.07,2\n",
        "0,540,3.5,2\n",
        "0,720,3.4,3\n",
        "0,640,3.3,2\n",
        "1,660,3.6,3\n",
        "1,400,3.15,2\n",
        "1,680,3.98,2\n",
        "0,220,2.83,3\n",
        "0,580,3.46,4\n",
        "1,540,3.17,1\n",
        "0,580,3.51,2\n",
        "0,540,3.13,2\n",
        "0,440,2.98,3\n",
        "0,560,4,3\n",
        "0,660,3.67,2\n",
        "0,660,3.77,3\n",
        "1,520,3.65,4\n",
        "0,540,3.46,4\n",
        "1,300,2.84,2\n",
        "1,340,3,2\n",
        "1,780,3.63,4\n",
        "1,480,3.71,4\n",
        "0,540,3.28,1\n",
        "0,460,3.14,3\n",
        "0,460,3.58,2\n",
        "0,500,3.01,4\n",
        "0,420,2.69,2\n",
        "0,520,2.7,3\n",
        "0,680,3.9,1\n",
        "0,680,3.31,2\n",
        "1,560,3.48,2\n",
        "0,580,3.34,2\n",
        "0,500,2.93,4\n",
        "0,740,4,3\n",
        "0,660,3.59,3\n",
        "0,420,2.96,1\n",
        "0,560,3.43,3\n",
        "1,460,3.64,3\n",
        "1,620,3.71,1\n",
        "0,520,3.15,3\n",
        "0,620,3.09,4\n",
        "0,540,3.2,1\n",
        "1,660,3.47,3\n",
        "0,500,3.23,4\n",
        "1,560,2.65,3\n",
        "0,500,3.95,4\n",
        "0,580,3.06,2\n",
        "0,520,3.35,3\n",
        "0,500,3.03,3\n",
        "0,600,3.35,2\n",
        "0,580,3.8,2\n",
        "0,400,3.36,2\n",
        "0,620,2.85,2\n",
        "1,780,4,2\n",
        "0,620,3.43,3\n",
        "1,580,3.12,3\n",
        "0,700,3.52,2\n",
        "1,540,3.78,2\n",
        "1,760,2.81,1\n",
        "0,700,3.27,2\n",
        "0,720,3.31,1\n",
        "1,560,3.69,3\n",
        "0,720,3.94,3\n",
        "1,520,4,1\n",
        "1,540,3.49,1\n",
        "0,680,3.14,2\n",
        "0,460,3.44,2\n",
        "1,560,3.36,1\n",
        "0,480,2.78,3\n",
        "0,460,2.93,3\n",
        "0,620,3.63,3\n",
        "0,580,4,1\n",
        "0,800,3.89,2\n",
        "1,540,3.77,2\n",
        "1,680,3.76,3\n",
        "1,680,2.42,1\n",
        "1,620,3.37,1\n",
        "0,560,3.78,2\n",
        "0,560,3.49,4\n",
        "0,620,3.63,2\n",
        "1,800,4,2\n",
        "0,640,3.12,3\n",
        "0,540,2.7,2\n",
        "0,700,3.65,2\n",
        "1,540,3.49,2\n",
        "0,540,3.51,2\n",
        "0,660,4,1\n",
        "1,480,2.62,2\n",
        "0,420,3.02,1\n",
        "1,740,3.86,2\n",
        "0,580,3.36,2\n",
        "0,640,3.17,2\n",
        "0,640,3.51,2\n",
        "1,800,3.05,2\n",
        "1,660,3.88,2\n",
        "1,600,3.38,3\n",
        "1,620,3.75,2\n",
        "1,460,3.99,3\n",
        "0,620,4,2\n",
        "0,560,3.04,3\n",
        "0,460,2.63,2\n",
        "0,700,3.65,2\n",
        "0,600,3.89,3\n",
        "```\n",
        "\n"
      ]
    }
  ]
}