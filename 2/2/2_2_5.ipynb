{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_2_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/2/2/2_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHA0OJ2Cf1iV",
        "colab_type": "text"
      },
      "source": [
        "##实现梯度下降\n",
        "现在我们知道了如何更新我们的权重：\n",
        "\n",
        "Δwij = η ∗ δj ∗ xi,\n",
        "\n",
        "你看到的是如何实现一次更新，那我们如何把代码转化为能够计算多次权重更新，使得我们的网络能够真正学习呢？\n",
        "\n",
        "作为示例，我们拿一个研究生学院录取数据，用梯度下降训练一个网络。数据可以在[这里](https://stats.idre.ucla.edu/stat/data/binary.csv)找到。数据有三个输入特征：GRE 分数、GPA 分数和本科院校排名（从 1 到 4）。排名 1 代表最好，排名 4 代表最差。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDHw0x.png)\n",
        "我们的目标是基于这些特征来预测一个学生能否被研究生院录取。这里，我们将使用有一个输出层的网络。用 sigmoid 做为激活函数。\n",
        "\n",
        "\n",
        "###数据清理\n",
        "\n",
        "你也许认为有三个输入单元，但实际上我们要先做数据转换。rank 是类别特征，其中的数字并不表示任何相对的值。排名第 2 并不是排名第 1 的两倍；排名第 3 也不是排名第 2 的 1.5 倍。因此，我们需要用 [dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) 来对 rank 进行编码。把数据分成 4 个新列，用 0 或 1 表示。排名为 1 的行对应 rank_1 列的值为 1 ，其余三列的值为 0；排名为 2 的行对应 rank_2 列的值为 1 ，其余三列的值为 0，以此类推。\n",
        "\n",
        "我们还需要把 GRE 和 GPA 数据标准化，也就是说使得它们的均值为 0，标准偏差为 1。因为 sigmoid 函数会挤压很大或者很小的输入，所以这一步是必要的。很大或者很小输入的梯度为 0，这意味着梯度下降的步长也会是 0。由于 GRE 和 GPA 的值都相当大，我们在初始化权重的时候需要非常小心，否则梯度下降步长将会消失，网络也没法训练了。相对地，如果我们对数据做了标准化处理，就能更容易地对权重进行初始化。\n",
        "\n",
        "这只是一个简单介绍，你之后还会学到如何预处理数据，如果你想了解我是怎么做的，可以查看下面编程练习中的 data_prep.py 文件。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDHIN8.png)\n",
        "经过转换后的 10 行数据\n",
        "\n",
        "现在数据已经准备好了，我们看到有六个输入特征：gre、gpa，以及四个 rank 的虚拟变量 （dummy variables）。\n",
        "\n",
        "\n",
        "###均方差\n",
        "\n",
        "这里我们要对如何计算误差做一点小改变。我们不计算 SSE，而是用误差平方的均值（mean of the square errors，MSE）。现在我们要处理很多数据，把所有权重更新加起来会导致很大的更新，使得梯度下降无法收敛。为了避免这种情况，你需要一个很小的学习率。这里我们还可以除以数据点的数量 mm 来取平均。这样，无论我们有多少数据，我们的学习率通常会在 0.01 to 0.001 之间。我们用 MSE（下图）来计算梯度，结果跟之前一样，只是取了平均而不是取和。\n",
        "\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDHXBq.png)\n",
        "你也可以对每条记录更新权重，而不是把所有记录都训练过之后再取平均。\n",
        "\n",
        "这里我们还是使用 sigmoid 作为激活函数\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDbPgJ.png)\n",
        "\n",
        "\n",
        "###用 NumPy 来实现\n",
        "\n",
        "这里大部分都可以用 NumPy 很方便的实现。\n",
        "\n",
        "首先你需要初始化权重。我们希望它们比较小，这样输入在 sigmoid 函数那里可以在接近 0 的位置，而不是最高或者最低处。很重要的一点是要随机地初始化它们，这样它们有不同的初始值，是发散且不对称的。所以我们用一个中心为 0 的正态分布来初始化权重，此正态分布的标准差（scale 参数）最好使用 1/sqrt(n)，其中 n 是输入单元的个数。这样就算是输入单元的数量变多，sigmoid 的输入还能保持比较小。\n",
        "\n",
        "\n",
        "```\n",
        "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
        "```\n",
        "\n",
        "NumPy 提供了一个可以让两个数组做点乘的函数，它可以让我们方便地计算 h。点乘就是把两个数组的元素对应位置相乘之后再相加。\n",
        "\n",
        "\n",
        "```\n",
        "# input to the output layer\n",
        "# 输出层的输入\n",
        "output_in = np.dot(weights, inputs)\n",
        "```\n",
        "\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDbBKs.png)\n",
        "\n",
        "\n",
        "###效率提示\n",
        "\n",
        "因为这里我们用的是 sigmoid 函数，你可以节省一些计算。对于 sigmoid 函数来说，f'(h) = f(h)(1 - f(h))。也就是说一旦你有了 f(h)，你就可以直接用它的值来计算误差的梯度了。\n",
        "\n",
        "\n",
        "###编程练习\n",
        "\n",
        "接下来，你要实现一个梯度下降，用录取数据来训练它。你的目标是训练一个网络直到你达到训练数据的最小的均方差 mean square error (MSE)。你需要实现：\n",
        "\n",
        "\n",
        "*   网络的输出: output\n",
        "*   输出误差: error\n",
        "*   误差项: error_term\n",
        "*   权重步长更新: del_w +=\n",
        "*   权重更新: weights +=\n",
        "\n",
        "\n",
        "在你写完这几部分之后，点击“测试答案”按钮来进行训练，均方差会被打印出来，同时也会打印出测试集的准确率，即录取情况的正确预测比率。\n",
        "\n",
        "你可以任意调节超参数 hyperparameters 来看下它对均方差 MSE 有什么影响。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K4k8X1Pji2A",
        "colab_type": "text"
      },
      "source": [
        "binary.csv\n",
        "\n",
        "```\n",
        "admit,gre,gpa,rank\n",
        "0,380,3.61,3\n",
        "1,660,3.67,3\n",
        "1,800,4,1\n",
        "1,640,3.19,4\n",
        "0,520,2.93,4\n",
        "1,760,3,2\n",
        "1,560,2.98,1\n",
        "0,400,3.08,2\n",
        "1,540,3.39,3\n",
        "0,700,3.92,2\n",
        "0,800,4,4\n",
        "0,440,3.22,1\n",
        "1,760,4,1\n",
        "0,700,3.08,2\n",
        "1,700,4,1\n",
        "0,480,3.44,3\n",
        "0,780,3.87,4\n",
        "0,360,2.56,3\n",
        "0,800,3.75,2\n",
        "1,540,3.81,1\n",
        "0,500,3.17,3\n",
        "1,660,3.63,2\n",
        "0,600,2.82,4\n",
        "0,680,3.19,4\n",
        "1,760,3.35,2\n",
        "1,800,3.66,1\n",
        "1,620,3.61,1\n",
        "1,520,3.74,4\n",
        "1,780,3.22,2\n",
        "0,520,3.29,1\n",
        "0,540,3.78,4\n",
        "0,760,3.35,3\n",
        "0,600,3.4,3\n",
        "1,800,4,3\n",
        "0,360,3.14,1\n",
        "0,400,3.05,2\n",
        "0,580,3.25,1\n",
        "0,520,2.9,3\n",
        "1,500,3.13,2\n",
        "1,520,2.68,3\n",
        "0,560,2.42,2\n",
        "1,580,3.32,2\n",
        "1,600,3.15,2\n",
        "0,500,3.31,3\n",
        "0,700,2.94,2\n",
        "1,460,3.45,3\n",
        "1,580,3.46,2\n",
        "0,500,2.97,4\n",
        "0,440,2.48,4\n",
        "0,400,3.35,3\n",
        "0,640,3.86,3\n",
        "0,440,3.13,4\n",
        "0,740,3.37,4\n",
        "1,680,3.27,2\n",
        "0,660,3.34,3\n",
        "1,740,4,3\n",
        "0,560,3.19,3\n",
        "0,380,2.94,3\n",
        "0,400,3.65,2\n",
        "0,600,2.82,4\n",
        "1,620,3.18,2\n",
        "0,560,3.32,4\n",
        "0,640,3.67,3\n",
        "1,680,3.85,3\n",
        "0,580,4,3\n",
        "0,600,3.59,2\n",
        "0,740,3.62,4\n",
        "0,620,3.3,1\n",
        "0,580,3.69,1\n",
        "0,800,3.73,1\n",
        "0,640,4,3\n",
        "0,300,2.92,4\n",
        "0,480,3.39,4\n",
        "0,580,4,2\n",
        "0,720,3.45,4\n",
        "0,720,4,3\n",
        "0,560,3.36,3\n",
        "1,800,4,3\n",
        "0,540,3.12,1\n",
        "1,620,4,1\n",
        "0,700,2.9,4\n",
        "0,620,3.07,2\n",
        "0,500,2.71,2\n",
        "0,380,2.91,4\n",
        "1,500,3.6,3\n",
        "0,520,2.98,2\n",
        "0,600,3.32,2\n",
        "0,600,3.48,2\n",
        "0,700,3.28,1\n",
        "1,660,4,2\n",
        "0,700,3.83,2\n",
        "1,720,3.64,1\n",
        "0,800,3.9,2\n",
        "0,580,2.93,2\n",
        "1,660,3.44,2\n",
        "0,660,3.33,2\n",
        "0,640,3.52,4\n",
        "0,480,3.57,2\n",
        "0,700,2.88,2\n",
        "0,400,3.31,3\n",
        "0,340,3.15,3\n",
        "0,580,3.57,3\n",
        "0,380,3.33,4\n",
        "0,540,3.94,3\n",
        "1,660,3.95,2\n",
        "1,740,2.97,2\n",
        "1,700,3.56,1\n",
        "0,480,3.13,2\n",
        "0,400,2.93,3\n",
        "0,480,3.45,2\n",
        "0,680,3.08,4\n",
        "0,420,3.41,4\n",
        "0,360,3,3\n",
        "0,600,3.22,1\n",
        "0,720,3.84,3\n",
        "0,620,3.99,3\n",
        "1,440,3.45,2\n",
        "0,700,3.72,2\n",
        "1,800,3.7,1\n",
        "0,340,2.92,3\n",
        "1,520,3.74,2\n",
        "1,480,2.67,2\n",
        "0,520,2.85,3\n",
        "0,500,2.98,3\n",
        "0,720,3.88,3\n",
        "0,540,3.38,4\n",
        "1,600,3.54,1\n",
        "0,740,3.74,4\n",
        "0,540,3.19,2\n",
        "0,460,3.15,4\n",
        "1,620,3.17,2\n",
        "0,640,2.79,2\n",
        "0,580,3.4,2\n",
        "0,500,3.08,3\n",
        "0,560,2.95,2\n",
        "0,500,3.57,3\n",
        "0,560,3.33,4\n",
        "0,700,4,3\n",
        "0,620,3.4,2\n",
        "1,600,3.58,1\n",
        "0,640,3.93,2\n",
        "1,700,3.52,4\n",
        "0,620,3.94,4\n",
        "0,580,3.4,3\n",
        "0,580,3.4,4\n",
        "0,380,3.43,3\n",
        "0,480,3.4,2\n",
        "0,560,2.71,3\n",
        "1,480,2.91,1\n",
        "0,740,3.31,1\n",
        "1,800,3.74,1\n",
        "0,400,3.38,2\n",
        "1,640,3.94,2\n",
        "0,580,3.46,3\n",
        "0,620,3.69,3\n",
        "1,580,2.86,4\n",
        "0,560,2.52,2\n",
        "1,480,3.58,1\n",
        "0,660,3.49,2\n",
        "0,700,3.82,3\n",
        "0,600,3.13,2\n",
        "0,640,3.5,2\n",
        "1,700,3.56,2\n",
        "0,520,2.73,2\n",
        "0,580,3.3,2\n",
        "0,700,4,1\n",
        "0,440,3.24,4\n",
        "0,720,3.77,3\n",
        "0,500,4,3\n",
        "0,600,3.62,3\n",
        "0,400,3.51,3\n",
        "0,540,2.81,3\n",
        "0,680,3.48,3\n",
        "1,800,3.43,2\n",
        "0,500,3.53,4\n",
        "1,620,3.37,2\n",
        "0,520,2.62,2\n",
        "1,620,3.23,3\n",
        "0,620,3.33,3\n",
        "0,300,3.01,3\n",
        "0,620,3.78,3\n",
        "0,500,3.88,4\n",
        "0,700,4,2\n",
        "1,540,3.84,2\n",
        "0,500,2.79,4\n",
        "0,800,3.6,2\n",
        "0,560,3.61,3\n",
        "0,580,2.88,2\n",
        "0,560,3.07,2\n",
        "0,500,3.35,2\n",
        "1,640,2.94,2\n",
        "0,800,3.54,3\n",
        "0,640,3.76,3\n",
        "0,380,3.59,4\n",
        "1,600,3.47,2\n",
        "0,560,3.59,2\n",
        "0,660,3.07,3\n",
        "1,400,3.23,4\n",
        "0,600,3.63,3\n",
        "0,580,3.77,4\n",
        "0,800,3.31,3\n",
        "1,580,3.2,2\n",
        "1,700,4,1\n",
        "0,420,3.92,4\n",
        "1,600,3.89,1\n",
        "1,780,3.8,3\n",
        "0,740,3.54,1\n",
        "1,640,3.63,1\n",
        "0,540,3.16,3\n",
        "0,580,3.5,2\n",
        "0,740,3.34,4\n",
        "0,580,3.02,2\n",
        "0,460,2.87,2\n",
        "0,640,3.38,3\n",
        "1,600,3.56,2\n",
        "1,660,2.91,3\n",
        "0,340,2.9,1\n",
        "1,460,3.64,1\n",
        "0,460,2.98,1\n",
        "1,560,3.59,2\n",
        "0,540,3.28,3\n",
        "0,680,3.99,3\n",
        "1,480,3.02,1\n",
        "0,800,3.47,3\n",
        "0,800,2.9,2\n",
        "1,720,3.5,3\n",
        "0,620,3.58,2\n",
        "0,540,3.02,4\n",
        "0,480,3.43,2\n",
        "1,720,3.42,2\n",
        "0,580,3.29,4\n",
        "0,600,3.28,3\n",
        "0,380,3.38,2\n",
        "0,420,2.67,3\n",
        "1,800,3.53,1\n",
        "0,620,3.05,2\n",
        "1,660,3.49,2\n",
        "0,480,4,2\n",
        "0,500,2.86,4\n",
        "0,700,3.45,3\n",
        "0,440,2.76,2\n",
        "1,520,3.81,1\n",
        "1,680,2.96,3\n",
        "0,620,3.22,2\n",
        "0,540,3.04,1\n",
        "0,800,3.91,3\n",
        "0,680,3.34,2\n",
        "0,440,3.17,2\n",
        "0,680,3.64,3\n",
        "0,640,3.73,3\n",
        "0,660,3.31,4\n",
        "0,620,3.21,4\n",
        "1,520,4,2\n",
        "1,540,3.55,4\n",
        "1,740,3.52,4\n",
        "0,640,3.35,3\n",
        "1,520,3.3,2\n",
        "1,620,3.95,3\n",
        "0,520,3.51,2\n",
        "0,640,3.81,2\n",
        "0,680,3.11,2\n",
        "0,440,3.15,2\n",
        "1,520,3.19,3\n",
        "1,620,3.95,3\n",
        "1,520,3.9,3\n",
        "0,380,3.34,3\n",
        "0,560,3.24,4\n",
        "1,600,3.64,3\n",
        "1,680,3.46,2\n",
        "0,500,2.81,3\n",
        "1,640,3.95,2\n",
        "0,540,3.33,3\n",
        "1,680,3.67,2\n",
        "0,660,3.32,1\n",
        "0,520,3.12,2\n",
        "1,600,2.98,2\n",
        "0,460,3.77,3\n",
        "1,580,3.58,1\n",
        "1,680,3,4\n",
        "1,660,3.14,2\n",
        "0,660,3.94,2\n",
        "0,360,3.27,3\n",
        "0,660,3.45,4\n",
        "0,520,3.1,4\n",
        "1,440,3.39,2\n",
        "0,600,3.31,4\n",
        "1,800,3.22,1\n",
        "1,660,3.7,4\n",
        "0,800,3.15,4\n",
        "0,420,2.26,4\n",
        "1,620,3.45,2\n",
        "0,800,2.78,2\n",
        "0,680,3.7,2\n",
        "0,800,3.97,1\n",
        "0,480,2.55,1\n",
        "0,520,3.25,3\n",
        "0,560,3.16,1\n",
        "0,460,3.07,2\n",
        "0,540,3.5,2\n",
        "0,720,3.4,3\n",
        "0,640,3.3,2\n",
        "1,660,3.6,3\n",
        "1,400,3.15,2\n",
        "1,680,3.98,2\n",
        "0,220,2.83,3\n",
        "0,580,3.46,4\n",
        "1,540,3.17,1\n",
        "0,580,3.51,2\n",
        "0,540,3.13,2\n",
        "0,440,2.98,3\n",
        "0,560,4,3\n",
        "0,660,3.67,2\n",
        "0,660,3.77,3\n",
        "1,520,3.65,4\n",
        "0,540,3.46,4\n",
        "1,300,2.84,2\n",
        "1,340,3,2\n",
        "1,780,3.63,4\n",
        "1,480,3.71,4\n",
        "0,540,3.28,1\n",
        "0,460,3.14,3\n",
        "0,460,3.58,2\n",
        "0,500,3.01,4\n",
        "0,420,2.69,2\n",
        "0,520,2.7,3\n",
        "0,680,3.9,1\n",
        "0,680,3.31,2\n",
        "1,560,3.48,2\n",
        "0,580,3.34,2\n",
        "0,500,2.93,4\n",
        "0,740,4,3\n",
        "0,660,3.59,3\n",
        "0,420,2.96,1\n",
        "0,560,3.43,3\n",
        "1,460,3.64,3\n",
        "1,620,3.71,1\n",
        "0,520,3.15,3\n",
        "0,620,3.09,4\n",
        "0,540,3.2,1\n",
        "1,660,3.47,3\n",
        "0,500,3.23,4\n",
        "1,560,2.65,3\n",
        "0,500,3.95,4\n",
        "0,580,3.06,2\n",
        "0,520,3.35,3\n",
        "0,500,3.03,3\n",
        "0,600,3.35,2\n",
        "0,580,3.8,2\n",
        "0,400,3.36,2\n",
        "0,620,2.85,2\n",
        "1,780,4,2\n",
        "0,620,3.43,3\n",
        "1,580,3.12,3\n",
        "0,700,3.52,2\n",
        "1,540,3.78,2\n",
        "1,760,2.81,1\n",
        "0,700,3.27,2\n",
        "0,720,3.31,1\n",
        "1,560,3.69,3\n",
        "0,720,3.94,3\n",
        "1,520,4,1\n",
        "1,540,3.49,1\n",
        "0,680,3.14,2\n",
        "0,460,3.44,2\n",
        "1,560,3.36,1\n",
        "0,480,2.78,3\n",
        "0,460,2.93,3\n",
        "0,620,3.63,3\n",
        "0,580,4,1\n",
        "0,800,3.89,2\n",
        "1,540,3.77,2\n",
        "1,680,3.76,3\n",
        "1,680,2.42,1\n",
        "1,620,3.37,1\n",
        "0,560,3.78,2\n",
        "0,560,3.49,4\n",
        "0,620,3.63,2\n",
        "1,800,4,2\n",
        "0,640,3.12,3\n",
        "0,540,2.7,2\n",
        "0,700,3.65,2\n",
        "1,540,3.49,2\n",
        "0,540,3.51,2\n",
        "0,660,4,1\n",
        "1,480,2.62,2\n",
        "0,420,3.02,1\n",
        "1,740,3.86,2\n",
        "0,580,3.36,2\n",
        "0,640,3.17,2\n",
        "0,640,3.51,2\n",
        "1,800,3.05,2\n",
        "1,660,3.88,2\n",
        "1,600,3.38,3\n",
        "1,620,3.75,2\n",
        "1,460,3.99,3\n",
        "0,620,4,2\n",
        "0,560,3.04,3\n",
        "0,460,2.63,2\n",
        "0,700,3.65,2\n",
        "0,600,3.89,3\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl-bugCGi2jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gradient.py#\n",
        "\n",
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
        "#       the previous lesson to encourage you to come up with a more\n",
        "#       efficient solution. If you need a hint, check out the comments\n",
        "#       in solution.py from the previous lecture.\n",
        "\n",
        "# Use to same seed to make debugging easier\n",
        "np.random.seed(42)\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "# Neural Network hyperparameters\n",
        "epochs = 1000\n",
        "learnrate = 0.5\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w = np.zeros(weights.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        # Loop through all records, x is the input, y is the target\n",
        "\n",
        "        # Note: We haven't included the h variable from the previous\n",
        "        #       lesson. You can add it if you want, or you can calculate\n",
        "        #       the h together with the output\n",
        "\n",
        "        # TODO: Calculate the output\n",
        "        output = sigmoid(np.dot(weights, x))\n",
        "\n",
        "        # TODO: Calculate the error\n",
        "        error = y - output\n",
        "\n",
        "        # TODO: Calculate the error term\n",
        "        error_term = error * output * (1 - output)\n",
        "\n",
        "        # TODO: Calculate the change in weights for this sample\n",
        "        #       and add it to the total weight change\n",
        "        del_w += error_term * x\n",
        "\n",
        "    # TODO: Update weights using the learning rate and the average change in weights\n",
        "    weights += learnrate * del_w / n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        out = sigmoid(np.dot(features, weights))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvW_mVkajCqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_prep.py#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "admissions = pd.read_csv('binary.csv')\n",
        "\n",
        "# Make dummy variables for rank\n",
        "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
        "data = data.drop('rank', axis=1)\n",
        "\n",
        "# Standarize features\n",
        "for field in ['gre', 'gpa']:\n",
        "    mean, std = data[field].mean(), data[field].std()\n",
        "    data.loc[:,field] = (data[field]-mean)/std\n",
        "    \n",
        "# Split off random 10% of the data for testing\n",
        "np.random.seed(42)\n",
        "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
        "data, test_data = data.ix[sample], data.drop(sample)\n",
        "\n",
        "# Split into features and targets\n",
        "features, targets = data.drop('admit', axis=1), data['admit']\n",
        "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jqAdmF9jQ2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution.py#\n",
        "\n",
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
        "#       the previous lesson to encourage you to come up with a more\n",
        "#       efficient solution. If you need a hint, check out the comments\n",
        "#       in solution.py from the previous lecture.\n",
        "\n",
        "# Use to same seed to make debugging easier\n",
        "np.random.seed(42)\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "# Neural Network hyperparameters\n",
        "epochs = 1000\n",
        "learnrate = 0.5\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w = np.zeros(weights.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        # Loop through all records, x is the input, y is the target\n",
        "\n",
        "        # Activation of the output unit\n",
        "        #   Notice we multiply the inputs and the weights here \n",
        "        #   rather than storing h as a separate variable \n",
        "        output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "        # The error, the target minus the network output\n",
        "        error = y - output\n",
        "\n",
        "        # The error term\n",
        "        #   Notice we calulate f'(h) here instead of defining a separate\n",
        "        #   sigmoid_prime function. This just makes it faster because we\n",
        "        #   can re-use the result of the sigmoid function stored in\n",
        "        #   the output variable\n",
        "        error_term = error * output * (1 - output)\n",
        "\n",
        "        # The gradient descent step, the error times the gradient times the inputs\n",
        "        del_w += error_term * x\n",
        "\n",
        "    # Update the weights here. The learning rate times the \n",
        "    # change in weights, divided by the number of records to average\n",
        "    weights += learnrate * del_w / n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        out = sigmoid(np.dot(features, weights))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}