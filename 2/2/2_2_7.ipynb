{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_2_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/2/2/2_2_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBn1ZZsTqFJn",
        "colab_type": "text"
      },
      "source": [
        "##反向传播\n",
        "\n",
        "如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。以一个两层神经网络为例，可以使用链式法则计算输入层-隐藏层间权重的误差。\n",
        "\n",
        "\n",
        "要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。\n",
        "\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDz3As.png)\n",
        "\n",
        "###范例\n",
        "\n",
        "以一个简单的两层神经网络为例，计算其权重的更新过程。假设该神经网络包含两个输入值，一个隐藏节点和一个输出节点，隐藏层和输出层的激活函数都是 sigmoid，如下图所示。（注意：图底部的节点为输入值，图顶部的 y^为输出值。输入层不计入层数，所以该结构被称为两层神经网络。）\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDzUjU.png)\n",
        "\n",
        "假设我们试着训练一些二进制数据，目标值是 y=1。我们从正向传播开始，首先计算输入到隐藏层节点\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDzbgf.png)\n",
        "\n",
        "然后，输入-隐藏层权重w(i)是学习速率乘以隐藏节点误差再乘以输入值。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KrSS5n.png)\n",
        "\n",
        "从这个例子中你可以看到 sigmoid 做激活函数的一个缺点。sigmoid 函数导数的最大值是 0.25，因此输出层的误差被减少了至少 75%，隐藏层的误差被减少了至少 93.75%！如果你的神经网络有很多层，使用 sigmoid 激活函数会很快把靠近输入层的权重步长降为很小的值，该问题称作梯度消失。后面的课程中你会学到在这方面表现更好，也被广泛用于最新神经网络中的其它激活函数。\n",
        "\n",
        "###用 NumPy 来实现\n",
        "\n",
        "现在你已经有了大部分用 NumPy 来实现反向传播的知识。\n",
        "\n",
        "但是之前接触的只是单个节点的误差项。现在在更新权重时，我们需要考虑隐藏层 每个节点 的误差 Δw(ij) = η * δ(j) * x(i)\t \n",
        "\n",
        "首先，会有不同数量的输入和隐藏节点，所以试图把误差与输入当作行向量来乘会报错\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "hidden_error*inputs\n",
        "---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "<ipython-input-22-3b59121cb809> in <module>()\n",
        "----> 1 hidden_error*x\n",
        "\n",
        "ValueError: operands could not be broadcast together with shapes (3,) (6,)\n",
        "```\n",
        "  \n",
        "  \n",
        "另外，现在w(ij)是一个矩阵，所以右侧对应也应该有跟左侧同样的维度。幸运的是，NumPy 这些都能搞定。如果你用一个列向量数组和一个行向量数组相乘，它会把列向量的第一个元素与行向量的每个元素相乘，组成一个新的二维数组的第一行。列向量的每一个元素依次重复该过程，最后你会得到一个二维数组，形状是 (len(column_vector), len(row_vector))。\n",
        "\n",
        "\n",
        "```\n",
        "hidden_error*inputs[:,None]\n",
        "array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],\n",
        "       [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],\n",
        "       [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],\n",
        "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
        "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
        "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])\n",
        "```\n",
        "\n",
        "\n",
        "这正好是我们计算权重更新的步长的方式。跟以前一样，如果你的输入是一个一行的二维数组，你可以用 hidden_error*inputs.T，但是如果 inputs 是一维数组，就不行了。\n",
        "\n",
        "\n",
        "###反向传播练习\n",
        "\n",
        "接下来你将用代码来实现一次两个权重的反向传播更新。我们提供了正向传播的代码，你来实现反向传播的部分。\n",
        "\n",
        "要做的事\n",
        "\n",
        "*   计算网络输出误差\n",
        "*   计算输出层误差项\n",
        "*   用反向传播计算隐藏层误差项\n",
        "*   计算反向传播误差的权重更新步长\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RasESrw2sRnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#backprop.py#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "x = np.array([0.5, 0.1, -0.2])\n",
        "target = 0.6\n",
        "learnrate = 0.5\n",
        "\n",
        "weights_input_hidden = np.array([[0.5, -0.6],\n",
        "                                 [0.1, -0.2],\n",
        "                                 [0.1, 0.7]])\n",
        "\n",
        "weights_hidden_output = np.array([0.1, -0.3])\n",
        "\n",
        "## Forward pass\n",
        "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "output = sigmoid(output_layer_in)\n",
        "\n",
        "## Backwards pass\n",
        "## TODO: Calculate output error\n",
        "error = None\n",
        "\n",
        "# TODO: Calculate error term for output layer\n",
        "output_error_term = None\n",
        "\n",
        "# TODO: Calculate error term for hidden layer\n",
        "hidden_error_term = None\n",
        "\n",
        "# TODO: Calculate change in weights for hidden layer to output layer\n",
        "delta_w_h_o = None\n",
        "\n",
        "# TODO: Calculate change in weights for input layer to hidden layer\n",
        "delta_w_i_h = None\n",
        "\n",
        "print('Change in weights for hidden layer to output layer:')\n",
        "print(delta_w_h_o)\n",
        "print('Change in weights for input layer to hidden layer:')\n",
        "print(delta_w_i_h)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALYvyucKsWol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution.py#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "x = np.array([0.5, 0.1, -0.2])\n",
        "target = 0.6\n",
        "learnrate = 0.5\n",
        "\n",
        "weights_input_hidden = np.array([[0.5, -0.6],\n",
        "                                 [0.1, -0.2],\n",
        "                                 [0.1, 0.7]])\n",
        "\n",
        "weights_hidden_output = np.array([0.1, -0.3])\n",
        "\n",
        "## Forward pass\n",
        "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "output = sigmoid(output_layer_in)\n",
        "\n",
        "## Backwards pass\n",
        "## TODO: Calculate output error\n",
        "error = target - output\n",
        "\n",
        "# TODO: Calculate error term for output layer\n",
        "output_error_term = error * output * (1 - output)\n",
        "\n",
        "# TODO: Calculate error term for hidden layer\n",
        "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
        "                    hidden_layer_output * (1 - hidden_layer_output)\n",
        "\n",
        "# TODO: Calculate change in weights for hidden layer to output layer\n",
        "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
        "\n",
        "# TODO: Calculate change in weights for input layer to hidden layer\n",
        "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
        "\n",
        "print('Change in weights for hidden layer to output layer:')\n",
        "print(delta_w_h_o)\n",
        "print('Change in weights for input layer to hidden layer:')\n",
        "print(delta_w_i_h)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}