{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_2_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/2/2/2_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC6xk54_7zf1",
        "colab_type": "text"
      },
      "source": [
        "##学习权重\n",
        "你了解了如何使用感知器来构建 AND 和 XOR 运算，但它们的权重都是人为设定的。如果你要进行一个运算，例如预测大学录取结果，但你不知道正确的权重是什么，该怎么办？你要从样本中学习权重，然后用这些权重来做预测。\n",
        "\n",
        "要了解我们将如何找到这些权重，可以从我们的目标开始考虑。我们想让网络做出的预测与真实值尽可能接近。为了能够衡量，我们需要有一个指标来了解预测有多差，也就是**误差 (error)**。一个普遍的指标是误差平方和 sum of the squared errors (SSE)：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDmoRg.png)\n",
        " \n",
        "\n",
        "这里 y^是预测值 y 是真实值。一个是所有输出单元 j 的和，另一个是所有数据点μ 的和。这里看上去很复杂，但你一旦理解了这些符号之后，你就能明白这是怎么回事了。\n",
        "\n",
        "首先是内部这个对 j 的求和。变量 j 代表网络输出单元。所以这个内部的求和是指对于每一个输出单元，计算预测值 y^与真实值 y 之间的差的平方，再求和。\n",
        "\n",
        "另一个对 μ 的求和是针对所有的数据点。也就是说，对每一个数据点，计算其对应输出单元的方差和，然后把每个数据点的方差和加在一起。这就是你整个输出的总误差。\n",
        "\n",
        "SSE 是一个很好的选择有几个原因：误差的平方总是正的，对大误差的惩罚大于小误差。同时，它对数学运算也更友好。\n",
        "\n",
        "回想神经网络的输出，也就是预测值，取决于权重\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDnpz4.png)\n",
        "\n",
        "我们想让网络预测的误差尽可能小，权重是让我们能够实现这个目标的调节旋钮。我们的目的是寻找权重 w(ij)使得误差平方 EE 最小。通常来说神经网络通过梯度下降来实现这一点。\n",
        "\n",
        "###梯度下降\n",
        "\n",
        "如 Luis 所说，用梯度下降，我们通过多个小步骤来实现目标。在这个例子中，我们希望一步一步改变权重来减小误差。借用前面的比喻，误差就像是山，我们希望走到山下。下山最快的路应该是最陡峭的那个方向，因此我们也应该寻找能够使误差最小化的方向。我们可以通过计算误差平方的梯度来找到这个方向。\n",
        "\n",
        "梯度是改变率或者斜度的另一个称呼。如果你需要回顾这个概念，可以看下可汗学院对这个问题的讲解。\n",
        "\n",
        "要计算变化率，我们要转向微积分，具体来说是导数。一个函数 f(x) 的导函数 f'(x)给到你的是 f(x) 在 x 这一点的斜率。例如 x^2，x^2的导数是 f'(x) = 2x。所以，在 x=2 这个点斜率 f'(2) = 4。画出图来就是：\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDnalQ.png)\n",
        "\n",
        "梯度示例\n",
        "\n",
        "梯度就是对多变量函数导数的泛化。我们可以用微积分来寻找误差函数中任意一点的梯度，它与输入权重有关，下一节你可以看到如何推导梯度下降的步骤。\n",
        "\n",
        "下面我画了一个拥有两个输入的神经网络误差示例，相应的，它有两个权重。你可以将其看成一个地形图，同一条线代表相同的误差，较深的线对应较大的误差。\n",
        "\n",
        "每一步，你计算误差和梯度，然后用它们来决定如何改变权重。重复这个过程直到你最终找到接近误差函数最小值的权重，即中间的黑点。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDnRl4.png)\n",
        "梯度下降到最小误差\n",
        "\n",
        "###注意事项\n",
        "因为权重会走向梯度带它去的位置，它们有可能停留在误差小，但不是最小的地方。这个点被称作局部最低点。如果权重初始值有错，梯度下降可能会使得权重陷入局部最优，例如下图所示。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDnO6H.png)\n",
        "梯度下降引向局部最低点\n",
        "\n",
        "有方法可以避免这一点，被称作 [momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum)."
      ]
    }
  ]
}