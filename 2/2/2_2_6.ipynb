{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_2_6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/2/2/2_2_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgoWdLKxkMVT",
        "colab_type": "text"
      },
      "source": [
        "##实现隐藏层\n",
        "####先修要求\n",
        "接下来我们会讲神经网络在多层感知器里面的数学部分。讲多层感知器我们会用到向量和矩阵。你可以通过下列讲解对此做个回顾：\n",
        "\n",
        "\n",
        "1.   可汗学院的 [向量入门](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra).\n",
        "2.   可汗学院的 [矩阵入门](https://www.khanacademy.org/math/precalculus/precalc-matrices).\n",
        "\n",
        "\n",
        "####由来\n",
        "之前我们研究的是只有一个输出节点网络，代码也很直观。但是现在我们有多个输入单元和多个隐藏单元，它们的权重需要有两个索引wij，其中 i 表示输入单元，j 表示隐藏单元。\n",
        "\n",
        "例如在下面这个网络图中，输入单元被标注为x1,x2,x3，隐藏层节点是 h1和 h2。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDOTZn.png)\n",
        "代表指向 h1的权重的线条被标成了红色，这样更好区分。\n",
        "\n",
        "为了定位权重，我们把输入节点的索引i和隐藏节点的索引j结合，得到：\n",
        "\n",
        "```w11```\n",
        "\n",
        "代表从 x1到 h 1的权重；\n",
        "\n",
        "```w12```\n",
        "\n",
        "代表从x1到h2的权重。\n",
        "\n",
        "下图包括了从输入层到隐藏层的所有权重，用w(ij)表示：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDXfFx.png)\n",
        "\n",
        "\n",
        "之前我们可以把权重写成数组，用 wi来索引。\n",
        "\n",
        "现在，权重被储存在矩阵中，由wij来索引。矩阵中的每一行对应从同一个输入节点发出的权重，每一列对应传入同一个隐藏节点的权重。这里我们有三个输入节点，两个隐藏节点，权重矩阵表示为：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDXjtP.png)\n",
        "三个输入节点两个隐藏节点的权重矩阵\n",
        "\n",
        "对比上面的示意图，确保你了解了不同的权重在矩阵中与在神经网络中的对应关系。\n",
        "\n",
        "用 NumPy 来初始化这些权重，我们需要提供矩阵的形状（shape），如果特征是一个包含以下数据的二维数组：\n",
        "\n",
        "\n",
        "```\n",
        "# Number of records and input units\n",
        "# 数据点数量以及每个数据点有多少输入节点\n",
        "n_records, n_inputs = features.shape\n",
        "# Number of hidden units\n",
        "# 隐藏层节点个数\n",
        "n_hidden = 2\n",
        "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "这样创建了一个名为 weights_input_to_hidden 的二维数组，维度是 n_inputs 乘 n_hidden。记住，输入层到隐藏层的节点是所有输入乘以隐藏节点权重的和。所以对每一个隐藏层节点 h(j)，我们需要计算：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDjwBd.png)\n",
        "\n",
        "为了实现这点，我们需要运用[矩阵乘法](https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%99%A3%E4%B9%98%E6%B3%95)，如果你对线性代数的知识有些模糊，我们建议你看下之前先修部分的资料。这里你只需要了解矩阵与向量如何相乘。\n",
        "\n",
        "在这里，我们把输入（一个行向量）与权重相乘。要实现这个，要把输入点乘（内积）以权重矩阵的每一列。例如要计算到第一个隐藏节点 j = 1j=1 的输入，你需要把这个输入与权重矩阵的第一列做点乘：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDjhHs.png)\n",
        "用输入与权重矩阵的第一列相乘得出到隐藏层第一个节点的输入\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDvC8K.png)\n",
        "针对第二个隐藏节点的输入，你需要计算输入与第二列的点积，以此类推。\n",
        "\n",
        "在 NumPy 中，你可以直接使用 np.dot\n",
        "\n",
        "```\n",
        "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
        "```\n",
        "\n",
        "你可以定义你的权重矩阵的维度是 n_hidden 乘 n_inputs 然后与列向量形式的输入相乘：\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDv1Kg.png)\n",
        "\n",
        "注意：\n",
        "\n",
        "这里权重的索引在上图中做了改变，与之前图片并不匹配。这是因为，在矩阵标注时行索引永远在列索引之前，所以用之前的方法做标识会引起误导。你只需要了解这跟之前的权重矩阵是一样的，只是做了转换，之前的第一列现在是第一行，之前的第二列现在是第二行。如果用之前的标记，权重矩阵是下面这个样子的：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDvYan.gif)\n",
        "用之前的标记标注的权重矩阵\n",
        "\n",
        "切记，上面标注方式是不正确的，这里只是为了让你更清楚这个矩阵如何跟之前神经网络的权重匹配。\n",
        "\n",
        "矩阵相乘非常重要的是他们的维度相匹配。因为它们在点乘时需要有相同数量的元素。在第一个例子中，输入向量有三列，权重矩阵有三行；第二个例子中，权重矩阵有三列，输入向量有三行。如果维度不匹配，你会得到：\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Same weights and features as above, but swapped the order\n",
        "hidden_inputs = np.dot(weights_input_to_hidden, features)\n",
        "---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "<ipython-input-11-1bfa0f615c45> in <module>()\n",
        "----> 1 hidden_in = np.dot(weights_input_to_hidden, X)\n",
        "\n",
        "ValueError: shapes (3,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "  \n",
        "3x2 的矩阵跟 3 个元素的数组是没法相乘的。因为矩阵中的两列与数组中的元素个数并不匹配。能够相乘的矩阵如下：\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KDvTZd.png)\n",
        "\n",
        "这里的规则是，如果是数组在左边，数组的元素个数必须与右边矩阵的行数一样。如果矩阵在左边，那么矩阵的列数，需要与右边向量的行数匹配。\n",
        "\n",
        "###构建一个列向量\n",
        "\n",
        "看上面的介绍，你有时会需要一个列向量，尽管 NumPy 默认是行向量。你可以用 arr.T 来对数组进行转置，但对一维数组来说，转置还是行向量。所以你可以用 arr[:,None] 来创建一个列向量：\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "print(features)\n",
        "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
        "\n",
        "print(features.T)\n",
        "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
        "\n",
        "print(features[:, None])\n",
        "> array([[ 0.49671415],\n",
        "       [-0.1382643 ],\n",
        "       [ 0.64768854]])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "当然，你可以创建一个二维数组，然后用 arr.T 得到列向量。\n",
        "\n",
        "```\n",
        "np.array(features, ndmin=2)\n",
        "> array([[ 0.49671415, -0.1382643 ,  0.64768854]])\n",
        "\n",
        "np.array(features, ndmin=2).T\n",
        "> array([[ 0.49671415],\n",
        "       [-0.1382643 ],\n",
        "       [ 0.64768854]])\n",
        "```\n",
        "       \n",
        "我个人更倾向于保持所有向量为一维数组，这样可以更好认知。\n",
        "\n",
        "###编程练习\n",
        "\n",
        "下面你要实现一个 4x3x2 网络的正向传播，用 sigmoid 作为两层的激活函数。\n",
        "\n",
        "要做的事情：\n",
        "\n",
        "*   计算隐藏层的输入\n",
        "*   计算隐藏层输出\n",
        "*   计算输出层的输入\n",
        "*   计算神经网络的输出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQr2IZvpdrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#solution.py#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Network size\n",
        "N_input = 4\n",
        "N_hidden = 3\n",
        "N_output = 2\n",
        "\n",
        "np.random.seed(42)\n",
        "# Make some fake data\n",
        "X = np.random.randn(4)\n",
        "\n",
        "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
        "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
        "\n",
        "\n",
        "# TODO: Make a forward pass through the network\n",
        "\n",
        "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
        "hidden_layer_out = sigmoid(hidden_layer_in)\n",
        "\n",
        "print('Hidden-layer Output:')\n",
        "print(hidden_layer_out)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
        "output_layer_out = sigmoid(output_layer_in)\n",
        "\n",
        "print('Output-layer Output:')\n",
        "print(output_layer_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DwXwvexpTSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#multilayer.py#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Network size\n",
        "N_input = 4\n",
        "N_hidden = 3\n",
        "N_output = 2\n",
        "\n",
        "np.random.seed(42)\n",
        "# Make some fake data\n",
        "X = np.random.randn(4)\n",
        "\n",
        "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
        "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
        "\n",
        "\n",
        "# TODO: Make a forward pass through the network\n",
        "\n",
        "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
        "hidden_layer_out = sigmoid(hidden_layer_in)\n",
        "\n",
        "print('Hidden-layer Output:')\n",
        "print(hidden_layer_out)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
        "output_layer_out = sigmoid(output_layer_in)\n",
        "\n",
        "print('Output-layer Output:')\n",
        "print(output_layer_out)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}