{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_12_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/6/12/6_12_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aco3QhX-zQtz",
        "colab_type": "text"
      },
      "source": [
        "##DDPG: 评论者\n",
        "\n",
        "###DDPG: Critic (Value) Model\n",
        "相应的评论者模型可以如下所示："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lrao7JzzNtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic:\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Initialize any other variables here\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
        "        # Define input layers\n",
        "        states = layers.Input(shape=(self.state_size,), name='states')\n",
        "        actions = layers.Input(shape=(self.action_size,), name='actions')\n",
        "\n",
        "        # Add hidden layer(s) for state pathway\n",
        "        net_states = layers.Dense(units=32, activation='relu')(states)\n",
        "        net_states = layers.Dense(units=64, activation='relu')(net_states)\n",
        "\n",
        "        # Add hidden layer(s) for action pathway\n",
        "        net_actions = layers.Dense(units=32, activation='relu')(actions)\n",
        "        net_actions = layers.Dense(units=64, activation='relu')(net_actions)\n",
        "\n",
        "        # Try different layer sizes, activations, add batch normalization, regularizers, etc.\n",
        "\n",
        "        # Combine state and action pathways\n",
        "        net = layers.Add()([net_states, net_actions])\n",
        "        net = layers.Activation('relu')(net)\n",
        "\n",
        "        # Add more layers to the combined network if needed\n",
        "\n",
        "        # Add final output layer to prduce action values (Q values)\n",
        "        Q_values = layers.Dense(units=1, name='q_values')(net)\n",
        "\n",
        "        # Create Keras model\n",
        "        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n",
        "\n",
        "        # Define optimizer and compile model for training with built-in loss function\n",
        "        optimizer = optimizers.Adam()\n",
        "        self.model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "        # Compute action gradients (derivative of Q values w.r.t. to actions)\n",
        "        action_gradients = K.gradients(Q_values, actions)\n",
        "\n",
        "        # Define an additional function to fetch action gradients (to be used by actor model)\n",
        "        self.get_action_gradients = K.function(\n",
        "            inputs=[*self.model.input, K.learning_phase()],\n",
        "            outputs=action_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXAqHzprzbdC",
        "colab_type": "text"
      },
      "source": [
        "它在某些方面比行动者模型简单，但是需要注意几点。首先，行动者模型旨在将状态映射到动作，而评论者模型需要将（状态、动作）对映射到它们的 Q 值。这一点体现在了输入层中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVds4Em9zeca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define input layers\n",
        "states = layers.Input(shape=(self.state_size,), name='states')\n",
        "actions = layers.Input(shape=(self.action_size,), name='actions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zN9EgDHzg9y",
        "colab_type": "text"
      },
      "source": [
        "这两个层级首先可以通过单独的“路径”（迷你子网络）处理，但是最终需要结合到一起。例如，可以通过使用 Keras 中的 Add 层级类型来实现[请参阅合并层级](https://keras.io/layers/merge/)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHRENdkjzp9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine state and action pathways\n",
        "net = layers.Add()([net_states, net_actions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUAknzGHzsdj",
        "colab_type": "text"
      },
      "source": [
        "该模型的最终输出是任何给定（状态、动作）对的 Q 值。但是，我们还需要计算此 Q 值相对于相应动作向量的梯度，以用于训练行动者模型。这一步需要明确执行，并且需要定义一个单独的函数来访问这些梯度："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUatGVzJzvUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute action gradients (derivative of Q values w.r.t. to actions)\n",
        "action_gradients = K.gradients(Q_values, actions)\n",
        "\n",
        "# Define an additional function to fetch action gradients (to be used by actor model)\n",
        "self.get_action_gradients = K.function(\n",
        "    inputs=[*self.model.input, K.learning_phase()],\n",
        "    outputs=action_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}