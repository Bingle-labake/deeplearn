{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_12_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/6/12/6_12_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT_j0GJHyUqA",
        "colab_type": "text"
      },
      "source": [
        "###深度确定性策略梯度 (DDPG):行动者\n",
        "你可以使用很多种不同的算法来设计智能体，只要它适合连续状态和动作空间即可。一种热门方法是深度确定性策略梯度，简称 DDPG。它实际上是一种行动者-评论者方法，但是关键在于底层的策略函数本身确定性函数，从外部添加了一些噪点，以便采取的动作具有理想的随机性。\n",
        "\n",
        "我们来实现原始论文中给出的算法：\n",
        "\n",
        "\n",
        "\n",
        "> Lillicrap, Timothy P等，2015. 深度强化学习连续控制. [pdf](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "\n",
        "\n",
        "\n",
        "可以使用最现代的深度学习库（例如 Keras 或 TensorFlow）实现该算法的两大组件 - 行动者和评论者网络。\n",
        "\n",
        "###DDPG：行动者（策略）模型\n",
        "以下是使用 Keras 定义的一个非常简单的行动者模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMExDDO5yTzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers, models, optimizers\n",
        "from keras import backend as K\n",
        "\n",
        "class Actor:\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, action_low, action_high):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            action_low (array): Min value of each action dimension\n",
        "            action_high (array): Max value of each action dimension\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_low = action_low\n",
        "        self.action_high = action_high\n",
        "        self.action_range = self.action_high - self.action_low\n",
        "\n",
        "        # Initialize any other variables here\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "        # Define input layer (states)\n",
        "        states = layers.Input(shape=(self.state_size,), name='states')\n",
        "\n",
        "        # Add hidden layers\n",
        "        net = layers.Dense(units=32, activation='relu')(states)\n",
        "        net = layers.Dense(units=64, activation='relu')(net)\n",
        "        net = layers.Dense(units=32, activation='relu')(net)\n",
        "\n",
        "        # Try different layer sizes, activations, add batch normalization, regularizers, etc.\n",
        "\n",
        "        # Add final output layer with sigmoid activation\n",
        "        raw_actions = layers.Dense(units=self.action_size, activation='sigmoid',\n",
        "            name='raw_actions')(net)\n",
        "\n",
        "        # Scale [0, 1] output for each action dimension to proper range\n",
        "        actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low,\n",
        "            name='actions')(raw_actions)\n",
        "\n",
        "        # Create Keras model\n",
        "        self.model = models.Model(inputs=states, outputs=actions)\n",
        "\n",
        "        # Define loss function using action value (Q value) gradients\n",
        "        action_gradients = layers.Input(shape=(self.action_size,))\n",
        "        loss = K.mean(-action_gradients * actions)\n",
        "\n",
        "        # Incorporate any additional losses here (e.g. from regularizers)\n",
        "\n",
        "        # Define optimizer and training function\n",
        "        optimizer = optimizers.Adam()\n",
        "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
        "        self.train_fn = K.function(\n",
        "            inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
        "            outputs=[],\n",
        "            updates=updates_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOQz_mRvytCy",
        "colab_type": "text"
      },
      "source": [
        "注意，输出层生成的原始动作位于[0.0, 1.0]范围内（使用 sigmoid 激活函数）。因此，我们添加另一个层级，该层级会针对每个动作维度将每个输出缩放到期望的范围。这样会针对任何给定状态向量生成确定性动作。稍后将向此动作添加噪点，以生成某个探索性行为。\n",
        "\n",
        "\n",
        "另一个需要注意的是损失函数如何使用动作值（Q 值）梯度进行定义："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgX3E2xOyxHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function using action value (Q value) gradients\n",
        "action_gradients = layers.Input(shape=(self.action_size,))\n",
        "loss = K.mean(-action_gradients * actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG7LCcqWyz7I",
        "colab_type": "text"
      },
      "source": [
        "这些梯度需要使用评论者模型计算，并在训练时提供梯度。因此指定为在训练函数中使用的“输入”的一部分："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps1SVWyCy2qA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "self.train_fn = K.function(\n",
        "   inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
        "    outputs=[],\n",
        "    updates=updates_op)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}