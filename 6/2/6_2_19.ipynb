{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_2_19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/6/2/6_2_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_bneKclI5td",
        "colab_type": "text"
      },
      "source": [
        "##总结\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/K0olm8.png)\n",
        "强化学习中的智能体环境互动。（来源：Sutton 和 Barto，2017 年）\n",
        "\n",
        "###设置，重新经历\n",
        "\n",
        "*   强化学习 (RL) 框架包含学习与其环境互动的智能体。\n",
        "*   在每个时间步，智能体都收到环境的状态（环境向智能体呈现一种情况），智能体必须选择相应的响应动作。一个时间步后，智能体获得一个奖励（环境表示智能体是否对该状态做出了正确的响应）和新的状态。\n",
        "*   所有智能体的目标都是最大化预期累积奖励，或在所有时间步获得的预期奖励之和。\n",
        "\n",
        "###阶段性任务与连续性任务\n",
        "\n",
        "*   任务是一种强化学习问题。\n",
        "\n",
        "*   连续性任务是一直持续下去、没有结束点的任务。\n",
        "\n",
        "*   阶段性任务是起始点和结束点明确的任务。\n",
        "\n",
        "> *   在这种情况下，我们将一个完整的互动系列（从开始到结束）称为一个阶段。\n",
        "\n",
        "> *   每当智能体抵达最终状态，阶段性任务都会结束。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###奖励假设\n",
        "*   奖励假设：所有目标都可以构建为最大化（预期）累积奖励。\n",
        "\n",
        "###目标和奖励\n",
        "*   （请参阅第 1 部分和第 2 部分，以查看在现实问题中如何指定奖励信号的示例。）\n",
        "\n",
        "###累积奖励\n",
        "*   在时间步 t 的回报是 G(t):=R(t+1) + R(t+2) + R(t+3) + …\n",
        "*   智能体选择动作的目标是最大化预期（折扣）回报。（注意：折扣将在下部分讲解。）\n",
        "\n",
        "###折扣回报\n",
        "*   在时间步 t 的折扣回报是 G(t):=R(t+1) + γ*R(t+2) + γ^2 * R(t+3) + …。\n",
        "*   折扣回报 γ 是你设置的值，以便进一步优化智能体的目标。\n",
        "> *   它必须指定 0≤γ≤1。\n",
        "> *   如果 γ=0，智能体只关心最即时的奖励。\n",
        "> *   如果 γ=1，回报没有折扣。\n",
        "> *   γ 的值越大，智能体越关心遥远的未来。γ 的值越小，折扣程度越大，在最极端的情况下，智能体只关心最即时的奖励。\n",
        "\n",
        "###MDPs和一步动态特性\n",
        "*   状态空间S是所有（非终止）状态的集合。\n",
        "> *   在阶段性任务中，我们使用S+表示所有状态集合，包括终止状态。\n",
        "*   动作空间 A是潜在动作的集合。 (此外，A(s)是指在状态s∈S的潜在动作集合。)\n",
        "*   (请参阅第 2 部分，了解如何在回收机器人示例中指定奖励信号。)\n",
        "*   环境的一步动态特性会判断环境在每个时间步如何决定状态和奖励。可以通过指定每个潜在 s',r,s,and a 的 p(s',r|s,a) ≐ P{S(t+1) = s′,R(t+1) = r ∣ S(t) = s,A(t) = a} 定义动态特性。\n",
        "*   一个（有限）马尔可夫决策过程 (MDP) 由以下各项定义：\n",
        "> *   一组（有限的）状态 S（对于阶段性任务，则是S+）\n",
        "> *   一组（有限的）动作 A\n",
        "> *   一组奖励 R\n",
        "> *   环境的一步动态特性\n",
        "> *   折扣率 γ∈[0,1]"
      ]
    }
  ]
}