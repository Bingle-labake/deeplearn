{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_7_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/6/7/6_7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WdcVYl4s9m8",
        "colab_type": "text"
      },
      "source": [
        "##项目说明\n",
        "对于此迷你项目，你将使用 OpenAI Gym 的 Taxi-v2 环境设计一个算法，并指导出租车智能体浏览一个小的网格世界。\n",
        "\n",
        "现在花时间在新窗口中打开下个部分的工作区。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KBP1Fs.gif)\n",
        "在新窗口中打开工作区。\n",
        "\n",
        "接着，通过点击新建终端打开终端。在配置环境时，你将看到一些输出结果。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KBiY4A.gif)\n",
        "打开新的终端。\n",
        "\n",
        "工作区包含三个文件：\n",
        "\n",
        "*   Agent.py：在此处开发强化学习智能体。这是你唯一需要修改的文件。\n",
        "*   Monitor.py：interact 函数测试智能体通过与环境互动达到的学习效果。\n",
        "*   main.py：在终端里运行此文件，以检查智能体的性能。\n",
        "\n",
        "在工作区里打开所有三个文件。\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/KBiBDS.gif)\n",
        "\n",
        "‘打开 agent.py、Monitor.py 和 main.py。’\n",
        "\n",
        "接着，通过在终端中执行 python main.py 运行 main.py。\n",
        "\n",
        "终端中运行 python main.py。\n",
        "\n",
        "当你运行 main.py 时，你在 Agent.py 中指定的智能体会与环境互动 20,000 个阶段。互动详情在 Monitor.py 中指定，它会返回两个变量：avg_rewards 和 best_avg_reward。\n",
        "\n",
        "*   avg_rewards 是一个双队列，其中 avg_rewards[i] 是智能体从阶段 i+1 到阶段 i+100（含）收集的平均（未折扣）回报。例如，avg_rewards[0] 是智能体在前 100 个阶段中收集的平均回报。\n",
        "*   best_avg_reward 是 avg_rewards 中的最大项。这是你在确定智能体在该任务中的效果时应该使用的最终得分。\n",
        "\n",
        "\n",
        "你的任务是修改 Agents.py 文件以改进智能体的性能。\n",
        "\n",
        "\n",
        "*   使用 __init__() 方法定义任何所需的实例变量。目前，我们定义了智能体可以采取的动作数量 (nA) ，并将动作值 (Q) 初始化为空的数组字典。你可以随意添加更多实例变量；例如，如果智能体使用 Epsilon 贪婪策略选择动作，你可能会发现有必要定义 ε 的值。\n",
        "*   select_action() 方法将环境状态作为输入，并返回智能体选择的动作。我们提供的默认代码会随机地选择动作。\n",
        "*   step() 方法将（state、action、reward、next_state）元组以及 done 变量作为输入，如果 ε 已结束，该变量将为 True。默认代码（你肯定需要更改！）会使上个状态动作对的值加一。你应该更改此方法，以使用抽取的经验元组更新智能体对问题的了解。\n",
        "修改该函数后，你只需运行 python main.py 以测试新智能体。\n",
        "\n",
        "虽然你可以实现你所选的任何算法，但是注意，你可以通过使用我们在课程中介绍的一些方法达到满意的性能。\n",
        "\n",
        "###评估性能\n",
        "OpenAI Gym 将“[解决](https://gym.openai.com/envs/Taxi-v1/)”该任务定义为在连续尝试 100 次以上之后平均回报为 9.7。\n",
        "\n",
        "虽然我们不会给此迷你项目打分，但是，建议你在连续尝试 100 次以上之后至少达到 9.1（best_avg_reward > 9.1）。\n",
        "\n",
        "###分享成果\n",
        "如果你对你的实现很满意，请在 MLND slack 的 #ml-reinforcement 通道中分享你的结果！你也可以在该 slack 通道中提问、获取实现提示、分享观点或寻找合作者！\n",
        "\n",
        "最后一步，为了与更广泛的 RL 社区分享你的观点，可能需要写一个总结并提交到 [OpenAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard)！"
      ]
    }
  ]
}