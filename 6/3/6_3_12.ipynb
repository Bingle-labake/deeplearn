{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_3_12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/6/3/6_3_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziWoliHZXIW8",
        "colab_type": "text"
      },
      "source": [
        "##练习：最优策略\n",
        "如果状态空间S 和动作空间A 是有限的，我们可以用表格表示最优动作值函数 q∗，每个可能的环境状态 s∈S 和动作 a∈A 对应一个策略。\n",
        "\n",
        "特定状态动作对 s,a 的值是智能体从状态 s 开始并采取动作 a，然后遵守最优策略 π∗\n",
        "所获得的预期回报。\n",
        "\n",
        "我们在下方为虚拟马尔可夫决策流程 (MDP) (where S={s1,s2,s3} 和 A={a1,a2,a3}) 填充了一些值。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/K0OWPU.png)\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/K0zBBd.png)\n",
        "\n",
        "为了构建最优策略，我们可以先在每行（或每个状态）中选择最大化动作值函数的项。\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/K0vzM4.png)\n",
        "\n",
        "\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/K0zGA1.png)\n",
        "\n",
        "\n",
        "其中 p,q≥0 以及 p + q = 1。\n",
        "\n",
        "\n",
        "问题\n",
        "思考另一个对应不同的最优动作值函数的不同 MDP。请使用该动作值函数回答以下问题。\n",
        "![替代文字](https://s2.ax1x.com/2019/10/26/K0xpL9.png)\n",
        "\n",
        "**练习题**\n",
        "\n",
        "以下哪些语句表示了最优动作值函数对应的潜在最优策略？\n",
        "\n",
        "\n",
        "    1. 智能体在状态 s_1 始终选择动作 a_3。\n",
        "\n",
        "    2. 智能体在状态 s_2 可以随意选择动作 a_1 或动作 a_2。\n",
        "\n",
        "    3. 智能体在状态 s_3 必须选择动作 a_1。\n",
        "\n"
      ]
    }
  ]
}