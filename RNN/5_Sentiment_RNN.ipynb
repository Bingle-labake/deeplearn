{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Sentiment_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bingle-labake/deeplearn/blob/master/RNN/5_Sentiment_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWXNaOkWAyuD",
        "colab_type": "text"
      },
      "source": [
        "##情感分析 RNN\n",
        "在此 notebook 中，你将实现一个情感分析递归神经网络。\n",
        "\n",
        "\n",
        "> 使用 RNN 比普通的前馈网络更准确，因为我们可以包含关于字词序列的信息。\n",
        "\n",
        "\n",
        "我们将使用影评数据集以及情感标签：positive 正面或 negative 负面。\n",
        "\n",
        "![替代文字](http://p0.qhimg.com/t013a7fa22d276a009a.png)\n",
        "\n",
        "###网络结构\n",
        "网络结构如下图所示：\n",
        "\n",
        "![替代文字](http://p3.qhimg.com/t01e7e71cba9a48565c.png)\n",
        "\n",
        "\n",
        "> ***首先，将字词传入嵌入层***。我们需要创建一个嵌入层，因为数据集有成千上万的字词，所以我们需要一种更高效的输入数据表示方式，而不是采用低效的独热编码向量。我们之前在 Word2Vec 课程中已经讲解过嵌入层。我们可以使用 Skip-gram Word2Vec 模型训练嵌入向量，并将这些嵌入向量当做输入。但是，直接添加一个嵌入层就足够了，模型能够自己学习不同的嵌入表。我们使用嵌入层的目的是降维，而不是学习语言表示法。\n",
        "\n",
        "> ***将输入字词传入嵌入层后，将新的嵌入传递给 LSTM 单元***。LSTM 单元将向网络中添加递归连接，并使我们能够包含关于影评数据的字词序列信息。\n",
        "\n",
        "> ***最后，LSTM 输出将传入 S 型输出层***。之所以使用 S 型函数，是因为 positive = 1，negative = 0，而 S 型函数将输出 0-1 之间的预测情感值。\n",
        "\n",
        "\n",
        "我们只关心 S 型函数的最后一个值，其他输出值都可以忽略。我们将通过比较最后一个时间步的输出和训练标签（正面或负面）来计算损失。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###加载并可视化数据¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejroRH8CJa1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwUxCdZXAmQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from text files\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGaPa8SECl5_",
        "colab_type": "text"
      },
      "source": [
        "###预处理数据\n",
        "构建神经网络的第一步是将数据整理成恰当的形状，然后传入网络中。因为我们将使用嵌入层，所以需要将每个字词表示为整数。还需要稍微清理数据。\n",
        "\n",
        "上面显示了示例影评数据。下面是数据处理步骤：\n",
        "\n",
        "\n",
        "> 我们需要删除句点和多余的空格。\n",
        "\n",
        "> 此外，你可能注意到了，影评用换行符 \\n 分隔。我将使用 \\n 作为分隔符，将文本拆分为单个影评。\n",
        "\n",
        "> 然后将所有影评组合成一个很长的字符串。\n",
        "\n",
        "首先删除所有标点。然后获取没有换行符的文本并拆分为单个字词。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJZpSlf4C2mi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14acb915-e1f1-4b2d-bc21-4d9ae830d7eb"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wawC-V_VC3mi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "203a2229-5e10-46dc-c615-5b5a30ab03f2"
      },
      "source": [
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()\n",
        "words[:10]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-ehphqIC-tg",
        "colab_type": "text"
      },
      "source": [
        "###对字词进行编码\n",
        "嵌入查询要求我们向网络中传入整数。最简单的方式是创建字典，将词汇表中的字词映射到整数。然后将每个影评转换为整数，这样就能传入网络中。\n",
        "\n",
        "\n",
        "> 练习：现在将字词编码为整数。构建一个将字词映射到整数的字典。稍后我们将使用 0 填充输入向量，所以整数应该从 1 开始，而不是从 0 开始。 将影评转换为整数，并将影评存储到新的 reviews_ints 列表中。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPt02sV6DHdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8xIEoDNDL4V",
        "colab_type": "text"
      },
      "source": [
        "####测试代码\n",
        "\n",
        "为了测试你是否正确地实现了字典，请输出词汇表中的唯一字词的数量，并输出第一个标记化影评的内容。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvmF4cIXDPzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d3459ef2-f512-4d43-de6e-5dc21b987dd3"
      },
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D9CeXnJDS7c",
        "colab_type": "text"
      },
      "source": [
        "###对标签进行编码\n",
        "标签为“positive”或“negative”。要在网络中使用标签，我们需要将它们转换为 0 和 1。\n",
        "\n",
        "\n",
        "\n",
        "> 练习：将标签从 positive 和 negative 分别转换为 1 和 0，并将它们放入新的 encoded_labels 列表中。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LcjgmGkDZbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4YLePv5DcZS",
        "colab_type": "text"
      },
      "source": [
        "###删除离群值\n",
        "为了使影评保持标准形状，还要执行一个预处理步骤。网络要求输入文本是标准大小，所以我们需要将影评变形为特定的长度。为了满足该要求，我们将完成两大步骤：\n",
        "\n",
        "\n",
        "1.   删除超长或超短的影评，即离群值 2.填充或截断剩余数据，使所有影评长度一样。\n",
        "\n",
        "![替代文字](http://p0.qhimg.com/t017e7b3384ace69fcc.png)\n",
        "\n",
        "在填充影评之前，先检查文本中是否有超长或超短的影评。离群值会干扰训练过程。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP4pR3gOD31S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "07cd52aa-d316-4f2e-a3e4-4cd3e264b7f7"
      },
      "source": [
        "# outlier review stats\n",
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azdqdkJQD65i",
        "colab_type": "text"
      },
      "source": [
        "我们的数据存在几个问题。有一个影评的长度为 0。最长的影评太长了，导致 RNN 需要完成多个训练步。我们需要删除超短的影评并截断超长的影评。这样就能删除离群值，并提高模型训练效率。\n",
        "\n",
        "\n",
        "\n",
        "> 练习：首先，从 reviews_ints 列表中删除长度为 0 的影评，并从 encoded_labels 中删除相应的标签。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMo0qL7LEB-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f2d7ece5-6800-4d78-8330-b9e32646da8f"
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQdRf8QMEFrq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "###填充序列\n",
        "对于很短和很长的影评，我们将通过填充或截断方式使影评保持特定长度。对于短于 seq_length 的影评，我们将用 0 填充它。对于长于 seq_length 的影评，我们将截取前 seq_length 个字词。对于我们的模型来说，建议将 seq_length 设为 200。\n",
        "\n",
        "\n",
        "> 练习：定义一个返回 features 数组的函数，该数组包含填充到标准大小的影评，之后我们会将该数组传入网络中。\n",
        "\n",
        "\n",
        "> 1.   数据应该来自 review_ints，因为我们想将整数传入网络中。\n",
        "> 2.   每行应该包含 seq_length 个元素。\n",
        "> 3.   对于少于 seq_length 个字词的影评，在左侧填充 0。也就是说，如果影评为 ['best', 'movie', 'ever']（用整数表示则为 [117, 18, 128]），填充后的行为 [0, 0, 0, ..., 0, 117, 18, 128]。\n",
        "> 4.   对于长于 seq_length 的影评，仅将前 seq_length 个字词作为特征向量。\n",
        "\n",
        "\n",
        "举个例子，如果 seq_length=10 并且输入影评为："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYgAHyQuEygC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[117, 18, 128]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt63IzkBE0fK",
        "colab_type": "text"
      },
      "source": [
        "生成的填充序列应该为："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhDcEzsWE1Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtfINRtSE5Ch",
        "colab_type": "text"
      },
      "source": [
        "####最终 features 数组应该为二维数组，行数等于影评数，列数等于指定的 seq_length。\n",
        "\n",
        "这种处理方式很重要，并且实现方法有多种。如果你要构建深度神经网络，就需要会预处理数据。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0KOcVW6E9VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    ## implement function\n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(reviews_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "     \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rt8d8vlFAHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "bf2cd865-f9ac-453d-feb9-f2abb6c8008f"
      },
      "source": [
        "# Test your implementation!\n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu7oTGoqFEpB",
        "colab_type": "text"
      },
      "source": [
        "###训练集、验证集和测试集\n",
        "准备好数据后，我们需要将数据拆分为训练集、验证集和测试集。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> 练习：创建训练集、验证集和测试集。\n",
        "\n",
        "> *   你需要分别为特征和标签创建这些数据集，例如创建 train_x 和 train_y。\n",
        "\n",
        "\n",
        "> *   定义一个拆分比例 split_frac，表示将数据集中的多少数据保留为训练集。通常设为 0.8 或 0.9。\n",
        "\n",
        "> *   将剩余数据一分为二，创建验证集和测试集。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2mxPsrQFgO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6c506618-d7af-4bf1-b038-b8ae7fea784c"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sa9ZWO7Fi1x",
        "colab_type": "text"
      },
      "source": [
        "####检查代码\n",
        "\n",
        "训练集、验证集和测试集的比例分别为 0.8、0.1、0.1，最终的特征数据形状应该如下所示："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CDGK_UKFphL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                    Feature Shapes:\n",
        "Train set: \t\t (20000, 200) \n",
        "Validation set: \t(2500, 200) \n",
        "Test set: \t\t  (2500, 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsqdf-F2FtYx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "###DataLoader 和批处理\n",
        "创建训练集、测试集和验证集后，我们可以按照以下两个步骤创建 DataLoader： 1.使用 TensorDataset 创建一种已知数据格式。TensorDataset 的参数包括输入数据集和目标数据集，并且第一个维度一样，然后创建一个数据集。 2.创建 DataLoader 并批处理训练、验证和测试张量数据集。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU9HtRAaFxwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "93711576-ac81-4fea-902b-35e039029078"
      },
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2cb3e3382ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_guqyqiF0aY",
        "colab_type": "text"
      },
      "source": [
        "这是创建生成器函数并将数据分成多批的替代方式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tn0s7SnF1Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unFN6UHnF5rS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "62e31038-b665-4aeb-e7fc-19a014a2b9ee"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,   102,    21,    51],\n",
            "        [    0,     0,     0,  ...,     9,    67,  4033],\n",
            "        [    0,     0,     0,  ...,   359,    44,     8],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,    39,    12,   709],\n",
            "        [    8,    13,   250,  ...,    56,   651,    53],\n",
            "        [    0,     0,     0,  ...,    15, 55614, 55615]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
            "        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIRc8CY-F_JZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "##在 PyTorch 中创建情感分析网络\n",
        "请在下面定义网络。\n",
        "\n",
        "![替代文字](http://p3.qhimg.com/t01e7e71cba9a48565c.png)\n",
        "\n",
        "层级结构如下图所示： 1.一个[嵌入层](https://pytorch.org/docs/stable/nn.html#embedding)：将字词标记（整数）转换为特定大小的嵌入。 2.一个[ LSTM](https://pytorch.org/docs/stable/nn.html#lstm) 层级：由 hidden_state 大小和层级数量定义 3.一个全连接输出层：将 LSTM 层级输出映射到期望的 output_size 4.一个 S 型激活层：将所有输出转换为 0-1 之间的值；仅返回最后一个 S 型函数输出值作为网络的输出。\n",
        "\n",
        "###嵌入层\n",
        "我们需要添加一个[嵌入层](https://pytorch.org/docs/stable/nn.html#embedding)，因为词汇表中有 74000 多个字词。用独热编码的形式表示这么多的类别效率太低了。所以我们将使用嵌入层并将该嵌入层当做查询表，而不是使用独热编码。我们可以使用 Word2Vec 训练嵌入层，然后加载它。但是也可以创建一个新的层级后仅用于降维，并让网络自己去学习权重。\n",
        "\n",
        "###LSTM 层级\n",
        "我们将向该递归网络中添加一个[ LSTM](https://pytorch.org/docs/stable/nn.html#lstm)，该LSTM 的参数包括 input_size、hidden_dim、层数、丢弃概率（针对多个层级之间的丢弃层），以及一个 batch_first 参数。\n",
        "\n",
        "通常，如果层级更多（2-3 个），网络效果将更好。添加更多层级使网络能够学习复杂的关系。\n",
        "\n",
        "\n",
        "\n",
        "> ***练习：***完成 __init__、forward 和 init_hidden 函数。\n",
        "\n",
        "\n",
        "\n",
        "注意：init_hidden 应该将 LSTM 层级的隐藏状态和单元状态全初始化为 0，并将它们移到 GPU 上（如果有的话）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF5TIXLDGyao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59726ca7-ca39-487f-d63b-759636c0e43d"
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLXTVyINGzVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        \n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # define all layers\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        " \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7rsH-UIG34n",
        "colab_type": "text"
      },
      "source": [
        "###实例化网络\n",
        "请在此部分实例化网络。首先定义超参数。\n",
        "\n",
        "*   vocab_size：词汇表的大小或输入（字词标记）的值范围。\n",
        "*   output_size：期望输出的大小；我们希望输出的类别分数数量（正面/负面）。\n",
        "*   embedding_dim：嵌入查询表的列数；嵌入大小。\n",
        "*   hidden_dim：隐藏层的 LSTM 单元数量。通常数量越多，效果越好。常见的值包括 128、256、512 等。\n",
        "*   n_layers：网络中的 LSTM 层级数量。通常在 1-3 层之间\n",
        "\n",
        "\n",
        "> 练习：定义模型超参数。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf5v9IxEHMO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "ad144fe9-0429-4493-bb54-362f84294a7e"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUzXrvVGHP4g",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "###训练\n",
        "下面是典型的训练代码。如果你想自己编写代码，可以删了所有这些代码并自己手动输入代码。还可以添加代码并按名称保存模型。\n",
        "\n",
        "\n",
        "\n",
        "> 我们将使用一种新的交叉熵损失，这种损失专门用于单个 S 型函数输出。BCELoss，即二元交叉熵损失，对在 0-1 之间的单个值应用交叉熵损失。\n",
        "\n",
        "\n",
        "还有以下超参数：\n",
        "\n",
        "\n",
        "*   lr：优化器的学习速率。\n",
        "*   epochs：遍历训练集的次数。\n",
        "*   clip：最大梯度值上限（防止梯度爆炸）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F83kxIuSHftU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0izNPiCHgZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "e17588e8-a123-4e2b-8fec-b1c7557487e4"
      },
      "source": [
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.714163... Val Loss: 0.651452\n",
            "Epoch: 1/4... Step: 200... Loss: 0.557441... Val Loss: 0.613568\n",
            "Epoch: 1/4... Step: 300... Loss: 0.654193... Val Loss: 0.681645\n",
            "Epoch: 1/4... Step: 400... Loss: 0.678328... Val Loss: 0.678126\n",
            "Epoch: 2/4... Step: 500... Loss: 0.494402... Val Loss: 0.671706\n",
            "Epoch: 2/4... Step: 600... Loss: 0.508051... Val Loss: 0.526870\n",
            "Epoch: 2/4... Step: 700... Loss: 0.606021... Val Loss: 0.553802\n",
            "Epoch: 2/4... Step: 800... Loss: 0.651838... Val Loss: 0.495164\n",
            "Epoch: 3/4... Step: 900... Loss: 0.294188... Val Loss: 0.479113\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.351133... Val Loss: 0.454691\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.266095... Val Loss: 0.479119\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.440874... Val Loss: 0.446265\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.177936... Val Loss: 0.525860\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.202405... Val Loss: 0.467553\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.428756... Val Loss: 0.568864\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.067685... Val Loss: 0.475021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPf4oSKhHlDL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "###测试\n",
        "有几种方式可以测试网络。\n",
        "\n",
        "\n",
        "*   测试数据效果：首先，看看训练过的模型在上面定义的所有测试数据上的效果。我们将计算测试数据的平均损失和准确率。\n",
        "*   对用户生成的数据进行推理：其次，检查能否一次输入一个示例影评（没有标签），并看看训练过的模型会预测出什么结果。像这样查看新的用户输入数据并预测输出标签称为推理。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-juu4DWHuWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9b7fa440-31d7-46d5-bdf8-00eb7f1d00fa"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.505\n",
            "Test accuracy: 0.808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W5JijCaHxbH",
        "colab_type": "text"
      },
      "source": [
        "###对测试影评进行推理\n",
        "你可以将该 test_review 更改为任何其他文本。读一读影评，然后自己判断是正面还是负面影评。再看看模型能否正确预测出影评的情感！\n",
        "\n",
        "\n",
        "> 练习：编写一个 predict 函数，它的参数包括训练过的网络、普通的 text_review，以及序列长度，并输出一段描述影评是正面还是负面影评的文字。\n",
        "> *   你可以使用你已经定义过的任何函数，或定义任何帮助你完成 predict 的辅助函数，但是定义函数的参数只需包含训练过的网络、文本影评和序列长度。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl1BVLViH--D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "affbe21d-0ee2-4230-ce10-04f6538671ae"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "# negative test review\n",
        "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
        "\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    # get rid of punctuation\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    # splitting by spaces\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "\n",
        "    return test_ints\n",
        "\n",
        "# test code and generate tokenized review\n",
        "test_ints = tokenize_review(test_review_neg)\n",
        "print(test_ints)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 247, 18, 10, 28, 108, 113, 14, 388, 2, 10, 181, 60, 273, 144, 11, 18, 68, 76, 113, 2, 1, 410, 14, 539]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWNUs3QJpzbI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "78d40c24-8bce-45a1-d426-e5fb99950dab"
      },
      "source": [
        "# test sequence padding\n",
        "seq_length=200\n",
        "features = pad_features(test_ints, seq_length)\n",
        "\n",
        "print(features)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   1 247  18  10  28\n",
            "  108 113  14 388   2  10 181  60 273 144  11  18  68  76 113   2   1 410\n",
            "   14 539]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZvbWKEdp2Hn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92af3e02-9316-441a-a70d-d9fd2cc8c9be"
      },
      "source": [
        "# test conversion to tensor and pass into your model\n",
        "feature_tensor = torch.from_numpy(features)\n",
        "print(feature_tensor.size())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnFsoP6pH_9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "    ''' Prints out whether a give review is predicted to be \n",
        "        positive or negative in sentiment, using a trained model.\n",
        "        \n",
        "        params:\n",
        "        net - A trained net \n",
        "        test_review - a review made of normal text and punctuation\n",
        "        sequence_length - the padded length of a review\n",
        "        '''\n",
        "    \n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    net.eval()\n",
        "    \n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze()) \n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review detected!\")\n",
        "    else:\n",
        "        print(\"Negative review detected.\")\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfMlO42rIEzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# positive test review\n",
        "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErxRw4y5IFfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8e6ae868-9cbe-447a-b4aa-41dff48f0621"
      },
      "source": [
        "# call function\n",
        "# try negative and positive reviews!\n",
        "seq_length=200\n",
        "predict(net, test_review_neg, seq_length)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value, pre-rounding: 0.005395\n",
            "Negative review detected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTfiEjQHIJXo",
        "colab_type": "text"
      },
      "source": [
        "###可以自己编写测试影评！\n",
        "训练好模型并创建 predict 函数后，你可以传入任何类型的文本，此模型将预测该文本具有正面情感还是负面情感。试着让此模型发挥最大作用，并看看它会将哪些字词当做正面字词，将哪些字词当做负面字词。\n",
        "\n",
        "稍后你将学习如何将这样的模型部署到生产环境中，让模型对用户输入到网络应用中的任何数据做出预测。"
      ]
    }
  ]
}